{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/segmented_trajectories.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "nzHcSSvNrWV0",
        "outputId": "631c5fe2-8d88-4607-e66b-8fc8b4f681ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          BaseDateTime       LAT        LON     MMSI PatternDescriptor  \\\n",
              "0  2022-03-31 00:00:17  26.11809 -80.148150  1056261        Stationary   \n",
              "1  2022-03-31 00:00:27  26.11809 -80.148148  1056261        Stationary   \n",
              "2  2022-03-31 00:00:37  26.11809 -80.148147  1056261        Stationary   \n",
              "3  2022-03-31 00:00:47  26.11809 -80.148145  1056261        Stationary   \n",
              "4  2022-03-31 00:00:57  26.11809 -80.148143  1056261        Stationary   \n",
              "\n",
              "   Pattern_High Speed  Pattern_Slow Movement  Pattern_Stationary       SOG  \\\n",
              "0                 NaN                    NaN                 1.0  0.100000   \n",
              "1                 NaN                    NaN                 1.0  0.083607   \n",
              "2                 NaN                    NaN                 1.0  0.067213   \n",
              "3                 NaN                    NaN                 1.0  0.050820   \n",
              "4                 NaN                    NaN                 1.0  0.034426   \n",
              "\n",
              "   SegmentID  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb357173-91f6-43a2-9fde-49d29e1c2be8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BaseDateTime</th>\n",
              "      <th>LAT</th>\n",
              "      <th>LON</th>\n",
              "      <th>MMSI</th>\n",
              "      <th>PatternDescriptor</th>\n",
              "      <th>Pattern_High Speed</th>\n",
              "      <th>Pattern_Slow Movement</th>\n",
              "      <th>Pattern_Stationary</th>\n",
              "      <th>SOG</th>\n",
              "      <th>SegmentID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-03-31 00:00:17</td>\n",
              "      <td>26.11809</td>\n",
              "      <td>-80.148150</td>\n",
              "      <td>1056261</td>\n",
              "      <td>Stationary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-03-31 00:00:27</td>\n",
              "      <td>26.11809</td>\n",
              "      <td>-80.148148</td>\n",
              "      <td>1056261</td>\n",
              "      <td>Stationary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.083607</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-03-31 00:00:37</td>\n",
              "      <td>26.11809</td>\n",
              "      <td>-80.148147</td>\n",
              "      <td>1056261</td>\n",
              "      <td>Stationary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.067213</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-03-31 00:00:47</td>\n",
              "      <td>26.11809</td>\n",
              "      <td>-80.148145</td>\n",
              "      <td>1056261</td>\n",
              "      <td>Stationary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.050820</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-03-31 00:00:57</td>\n",
              "      <td>26.11809</td>\n",
              "      <td>-80.148143</td>\n",
              "      <td>1056261</td>\n",
              "      <td>Stationary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.034426</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb357173-91f6-43a2-9fde-49d29e1c2be8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cb357173-91f6-43a2-9fde-49d29e1c2be8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cb357173-91f6-43a2-9fde-49d29e1c2be8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2fe66a8c-89a6-4e79-8ad0-e52af73c3d16\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2fe66a8c-89a6-4e79-8ad0-e52af73c3d16')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2fe66a8c-89a6-4e79-8ad0-e52af73c3d16 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 23780,\n  \"fields\": [\n    {\n      \"column\": \"BaseDateTime\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"2022-03-31 00:00:51\",\n          \"2022-03-31 00:01:47\",\n          \"2022-03-31 00:02:32\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LAT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.41273307929665,\n        \"min\": 17.33234,\n        \"max\": 49.57992,\n        \"num_unique_values\": 15448,\n        \"samples\": [\n          18.894856703296703,\n          30.049167142857144,\n          29.96759855072464\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LON\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.61409152616169,\n        \"min\": -158.24993,\n        \"max\": -64.03596999999999,\n        \"num_unique_values\": 15852,\n        \"samples\": [\n          -81.62841718309859,\n          -94.89116845070424,\n          -80.7338625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MMSI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57214211,\n        \"min\": 1056261,\n        \"max\": 985346207,\n        \"num_unique_values\": 1482,\n        \"samples\": [\n          316030644,\n          367007470,\n          367706830\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PatternDescriptor\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Stationary\",\n          \"High Speed\",\n          \"Slow Movement\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pattern_High Speed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pattern_Slow Movement\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pattern_Stationary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SOG\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.52737509575509,\n        \"min\": 0.0,\n        \"max\": 102.3,\n        \"num_unique_values\": 5301,\n        \"samples\": [\n          1.246031746031746\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SegmentID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 686,\n        \"min\": 0,\n        \"max\": 2377,\n        \"num_unique_values\": 2378,\n        \"samples\": [\n          2337\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Pattern_High Speed'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj5DXAyer2xs",
        "outputId": "9e5555ad-da54-43cb-c88a-67b5ee8bef0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan,  1.])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Pattern_High Speed',\t'Pattern_Slow Movement']] = df[['Pattern_High Speed',\t'Pattern_Slow Movement']].fillna(0)"
      ],
      "metadata": {
        "id": "QFs9US2urer6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MhGTo26repG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PydG0Fnhrem-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v2Yd8-CIrefP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # Correct import of tqdm\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def k_fold_sequence_to_sequence(df, window_size, prediction_horizon, k=5, resample_interval=10):\n",
        "    \"\"\"\n",
        "    Apply K-Fold Cross-Validation for sequence-to-sequence data preparation.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Pandas DataFrame containing AIS data with columns 'BaseDateTime', 'LAT', 'LON', 'SOG', and one-hot encoded pattern descriptors.\n",
        "    - window_size: The number of data points in each input sequence (length of observed states X_k,).\n",
        "    - prediction_horizon: The number of time steps to predict (length of target sequence Y_k,h).\n",
        "    - k: Number of folds for cross-validation.\n",
        "    - resample_interval: Resampling interval for the AIS data, in seconds (assumed to be already applied in previous steps).\n",
        "\n",
        "    Returns:\n",
        "    - folds_train: List of training data for each fold (X_train, Y_train).\n",
        "    - folds_val: List of validation data for each fold (X_val, Y_val).\n",
        "    - journey_descriptors: List of journey descriptors for each sequence.\n",
        "    \"\"\"\n",
        "    folds_train = []\n",
        "    folds_val = []\n",
        "    journey_descriptors = []\n",
        "\n",
        "    # Step 1: Prepare the K-Fold split\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    # Group by MMSI\n",
        "    grouped = df.groupby('MMSI')\n",
        "\n",
        "    # Store all sequences in a list (we will split this into k-folds)\n",
        "    all_sequences_X = []\n",
        "    all_sequences_Y = []\n",
        "    all_journey_descriptors = []\n",
        "\n",
        "    # Process each MMSI group\n",
        "    for mmsi, group in tqdm(grouped, desc=\"Processing MMSI\", unit=\"MMSI\"):  # Corrected tqdm usage\n",
        "        group = group.sort_values(by='BaseDateTime')\n",
        "        group['BaseDateTime'] = pd.to_datetime(group['BaseDateTime'])\n",
        "\n",
        "        # Check if there's enough data for the sequence-to-sequence model\n",
        "        if len(group) < window_size + prediction_horizon:\n",
        "            continue  # Skip this MMSI if it doesn't have enough data\n",
        "\n",
        "        # Step 2: One-hot encode the pattern descriptors\n",
        "        one_hot_columns = ['Pattern_Stationary', 'Pattern_Slow Movement', 'Pattern_High Speed']\n",
        "\n",
        "        # Step 3: Create sliding windows for sequences\n",
        "        for start in range(0, len(group) - window_size - prediction_horizon + 1):\n",
        "            # X_k, input sequence: past `window_size` time steps\n",
        "            X_seq = group.iloc[start:start + window_size][['LAT', 'LON', 'SOG'] + one_hot_columns].values\n",
        "\n",
        "            # Y_k,h output sequence: next `prediction_horizon` time steps\n",
        "            Y_seq = group.iloc[start + window_size:start + window_size + prediction_horizon][['LAT', 'LON', 'SOG']].values\n",
        "\n",
        "            # Append the sequences and the journey descriptor\n",
        "            all_sequences_X.append(X_seq)\n",
        "            all_sequences_Y.append(Y_seq)\n",
        "            all_journey_descriptors.append(group['PatternDescriptor'].iloc[start + window_size])\n",
        "\n",
        "    # Convert sequences into numpy arrays for model training\n",
        "    all_sequences_X = np.array(all_sequences_X)\n",
        "    all_sequences_Y = np.array(all_sequences_Y)\n",
        "    all_journey_descriptors = np.array(all_journey_descriptors)\n",
        "\n",
        "    # Step 4: Perform K-Fold Cross Validation\n",
        "    for train_index, val_index in kf.split(all_sequences_X):\n",
        "        # Split into training and validation sets for each fold\n",
        "        X_train, X_val = all_sequences_X[train_index], all_sequences_X[val_index]\n",
        "        Y_train, Y_val = all_sequences_Y[train_index], all_sequences_Y[val_index]\n",
        "\n",
        "        # Store training and validation sets for each fold\n",
        "        folds_train.append((X_train, Y_train))\n",
        "        folds_val.append((X_val, Y_val))\n",
        "\n",
        "        # Store journey descriptors for each fold\n",
        "        journey_descriptors.append(all_journey_descriptors[val_index])\n",
        "\n",
        "    return folds_train, folds_val, journey_descriptors\n",
        "\n",
        "\n",
        "# Define the window size, prediction horizon, and the number of folds\n",
        "window_size = 10  # Number of observed states\n",
        "prediction_horizon = 5  # Number of time steps to predict\n",
        "k = 5  # Number of folds\n",
        "\n",
        "# Load the data from the CSV file (already preprocessed)\n",
        "file_path = '/content/segmented_trajectories.csv'\n",
        "ais_data = pd.read_csv(file_path)\n",
        "\n",
        "# Apply K-Fold Cross-Validation to prepare data\n",
        "folds_train, folds_val, journey_descriptors = k_fold_sequence_to_sequence(\n",
        "    ais_data, window_size, prediction_horizon, k=k\n",
        ")\n",
        "\n",
        "# Print out the shapes of the sequences for the first fold\n",
        "X_train, Y_train = folds_train[0]\n",
        "X_val, Y_val = folds_val[0]\n",
        "\n",
        "print(f\"Training Input Shape: {X_train.shape}\")\n",
        "print(f\"Training Output Shape: {Y_train.shape}\")\n",
        "print(f\"Validation Input Shape: {X_val.shape}\")\n",
        "print(f\"Validation Output Shape: {Y_val.shape}\")\n"
      ],
      "metadata": {
        "id": "hsf44ZJJgrdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06282718-230d-41a2-ffcf-f35dd1d3d4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing MMSI: 100%|██████████| 1482/1482 [00:09<00:00, 151.72MMSI/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Input Shape: (4537, 10, 6)\n",
            "Training Output Shape: (4537, 5, 3)\n",
            "Validation Input Shape: (1135, 10, 6)\n",
            "Validation Output Shape: (1135, 5, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0SwXmDkYLqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def k_fold_sequence_to_sequence(df, window_size, prediction_horizon, k=5):\n",
        "    \"\"\"\n",
        "    Apply K-Fold Cross-Validation for sequence-to-sequence data preparation with scaling.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Pandas DataFrame containing AIS data.\n",
        "    - window_size: Number of time steps in each input sequence.\n",
        "    - prediction_horizon: Number of time steps to predict.\n",
        "    - k: Number of folds for cross-validation.\n",
        "\n",
        "    Returns:\n",
        "    - folds_train: List of training data for each fold (X_train, Y_train).\n",
        "    - folds_val: List of validation data for each fold (X_val, Y_val).\n",
        "    - journey_descriptors: List of journey descriptors for validation sequences.\n",
        "    \"\"\"\n",
        "    folds_train = []\n",
        "    folds_val = []\n",
        "    journey_descriptors = []\n",
        "\n",
        "    # Group by MMSI\n",
        "    grouped = df.groupby('MMSI')\n",
        "\n",
        "    # Store all sequences\n",
        "    all_sequences_X = []\n",
        "    all_sequences_Y = []\n",
        "    all_journey_descriptors = []\n",
        "\n",
        "    # Define features for scaling\n",
        "    continuous_features = ['LAT', 'LON', 'SOG']\n",
        "\n",
        "    # Initialize scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the entire dataset\n",
        "    df_continuous = df[continuous_features]\n",
        "    scaler.fit(df_continuous)\n",
        "\n",
        "    # Process each MMSI group\n",
        "    for mmsi, group in tqdm(grouped, desc=\"Processing MMSI\", unit=\"MMSI\"):\n",
        "        group = group.sort_values(by='BaseDateTime')\n",
        "        group['BaseDateTime'] = pd.to_datetime(group['BaseDateTime'])\n",
        "\n",
        "        # Scale continuous features\n",
        "        group[continuous_features] = scaler.transform(group[continuous_features])\n",
        "\n",
        "        # Check if the group has enough data for the sequence-to-sequence model\n",
        "        if len(group) < window_size + prediction_horizon:\n",
        "            continue\n",
        "\n",
        "        # Generate sliding windows\n",
        "        for start in range(0, len(group) - window_size - prediction_horizon + 1):\n",
        "            # Input sequence\n",
        "            X_seq = group.iloc[start:start + window_size][continuous_features].values\n",
        "\n",
        "            # Output sequence\n",
        "            Y_seq = group.iloc[start + window_size:start + window_size + prediction_horizon][continuous_features].values\n",
        "\n",
        "            # Append to sequences\n",
        "            all_sequences_X.append(X_seq)\n",
        "            all_sequences_Y.append(Y_seq)\n",
        "            all_journey_descriptors.append(group['PatternDescriptor'].iloc[start + window_size])\n",
        "\n",
        "    # Convert sequences into numpy arrays\n",
        "    all_sequences_X = np.array(all_sequences_X)\n",
        "    all_sequences_Y = np.array(all_sequences_Y)\n",
        "    all_journey_descriptors = np.array(all_journey_descriptors)\n",
        "\n",
        "    # Apply K-Fold Cross Validation\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    for train_index, val_index in kf.split(all_sequences_X):\n",
        "        X_train, X_val = all_sequences_X[train_index], all_sequences_X[val_index]\n",
        "        Y_train, Y_val = all_sequences_Y[train_index], all_sequences_Y[val_index]\n",
        "        folds_train.append((X_train, Y_train))\n",
        "        folds_val.append((X_val, Y_val))\n",
        "        journey_descriptors.append(all_journey_descriptors[val_index])\n",
        "\n",
        "    return folds_train, folds_val, journey_descriptors\n",
        "\n",
        "\n",
        "# Define parameters\n",
        "window_size = 10  # Number of observed states\n",
        "prediction_horizon = 5  # Number of time steps to predict\n",
        "k = 5  # Number of folds\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/segmented_trajectories.csv'\n",
        "ais_data = pd.read_csv(file_path)\n",
        "ais_data[['Pattern_High Speed',\t'Pattern_Slow Movement']] = ais_data[['Pattern_High Speed',\t'Pattern_Slow Movement']].fillna(0)\n",
        "\n",
        "# Ensure proper datetime formatting\n",
        "ais_data['BaseDateTime'] = pd.to_datetime(ais_data['BaseDateTime'])\n",
        "\n",
        "# Check for and handle null values\n",
        "if ais_data.isnull().any().any():\n",
        "    print(\"Null values detected. Dropping rows with null values...\")\n",
        "    ais_data = ais_data.dropna()\n",
        "    print(f\"Remaining rows after dropping nulls: {len(ais_data)}\")\n",
        "\n",
        "# Apply K-Fold Cross-Validation with scaling\n",
        "folds_train, folds_val, journey_descriptors = k_fold_sequence_to_sequence(\n",
        "    ais_data, window_size, prediction_horizon, k=k\n",
        ")\n",
        "\n",
        "# Output the shapes of the sequences for the first fold\n",
        "X_train, Y_train = folds_train[0]\n",
        "X_val, Y_val = folds_val[0]\n",
        "\n",
        "print(f\"Training Input Shape: {X_train.shape}\")\n",
        "print(f\"Training Output Shape: {Y_train.shape}\")\n",
        "print(f\"Validation Input Shape: {X_val.shape}\")\n",
        "print(f\"Validation Output Shape: {Y_val.shape}\")\n",
        "\n",
        "\n",
        "# Check for NaN or infinite values in training and validation data\n",
        "print(f\"NaN in X_train: {np.isnan(X_train).any()}\")\n",
        "print(f\"NaN in Y_train: {np.isnan(Y_train).any()}\")\n",
        "print(f\"NaN in X_val: {np.isnan(X_val).any()}\")\n",
        "print(f\"NaN in Y_val: {np.isnan(Y_val).any()}\")\n",
        "\n",
        "print(f\"Infinite in X_train: {np.isinf(X_train).any()}\")\n",
        "print(f\"Infinite in Y_train: {np.isinf(Y_train).any()}\")\n",
        "print(f\"Infinite in X_val: {np.isinf(X_val).any()}\")\n",
        "print(f\"Infinite in Y_val: {np.isinf(Y_val).any()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wsWJZXhYLnT",
        "outputId": "633c4204-2ae8-45ae-9152-568a50eba3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null values detected. Dropping rows with null values...\n",
            "Remaining rows after dropping nulls: 16035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing MMSI: 100%|██████████| 982/982 [00:09<00:00, 101.55MMSI/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Input Shape: (3118, 10, 3)\n",
            "Training Output Shape: (3118, 5, 3)\n",
            "Validation Input Shape: (780, 10, 3)\n",
            "Validation Output Shape: (780, 5, 3)\n",
            "NaN in X_train: False\n",
            "NaN in Y_train: False\n",
            "NaN in X_val: False\n",
            "NaN in Y_val: False\n",
            "Infinite in X_train: False\n",
            "Infinite in Y_train: False\n",
            "Infinite in X_val: False\n",
            "Infinite in Y_val: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# Define the Seq2Seq model with Attention\n",
        "class Seq2SeqWithAttention(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, latent_dim, num_layers, timesteps_input, timesteps_output):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.timesteps_input = timesteps_input\n",
        "        self.timesteps_output = timesteps_output\n",
        "\n",
        "        self.encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "        self.decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "\n",
        "        self.attention_dense = tf.keras.layers.Dense(1)\n",
        "        self.attention_softmax = tf.keras.layers.Softmax(axis=1)\n",
        "\n",
        "        self.output_dense = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "\n",
        "        # Encoder\n",
        "        encoder_outputs, state_h, state_c = self.encoder_lstm(encoder_inputs)  # (batch_size, timesteps_input, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_outputs, _, _ = self.decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])  # (batch_size, timesteps_decoder, latent_dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = self.attention_score(encoder_outputs, decoder_outputs)  # (batch_size, timesteps_decoder, timesteps_input)\n",
        "\n",
        "        # Compute context vector\n",
        "        attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)  # (batch_size, timesteps_decoder, timesteps_input, 1)\n",
        "        encoder_outputs_expanded = tf.expand_dims(encoder_outputs, axis=1)  # (batch_size, 1, timesteps_input, latent_dim)\n",
        "        context_vector = tf.reduce_sum(encoder_outputs_expanded * attention_weights_expanded, axis=2)  # (batch_size, timesteps_decoder, latent_dim)\n",
        "\n",
        "        # Combine context vector and decoder outputs\n",
        "        decoder_combined_context = tf.concat([context_vector, decoder_outputs], axis=-1)  # (batch_size, timesteps_decoder, 2 * latent_dim)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = self.output_dense(decoder_combined_context)  # (batch_size, timesteps_decoder, output_dim)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def attention_score(self, encoder_outputs, decoder_outputs):\n",
        "        timesteps_decoder = tf.shape(decoder_outputs)[1]\n",
        "\n",
        "        # Tile decoder outputs to match the encoder sequence length\n",
        "        decoder_expanded = tf.expand_dims(decoder_outputs, axis=2)  # (batch_size, timesteps_decoder, 1, latent_dim)\n",
        "        decoder_tiled = tf.tile(decoder_expanded, [1, 1, self.timesteps_input, 1])  # (batch_size, timesteps_decoder, timesteps_input, latent_dim)\n",
        "\n",
        "        # Expand encoder outputs to match decoder timesteps\n",
        "        encoder_expanded = tf.expand_dims(encoder_outputs, axis=1)  # (batch_size, 1, timesteps_input, latent_dim)\n",
        "        encoder_tiled = tf.tile(encoder_expanded, [1, timesteps_decoder, 1, 1])  # (batch_size, timesteps_decoder, timesteps_input, latent_dim)\n",
        "\n",
        "        # Concatenate encoder and decoder outputs\n",
        "        concat = tf.concat([encoder_tiled, decoder_tiled], axis=-1)  # (batch_size, timesteps_decoder, timesteps_input, 2 * latent_dim)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = self.attention_dense(concat)  # (batch_size, timesteps_decoder, timesteps_input, 1)\n",
        "        attention_scores = tf.squeeze(attention_scores, axis=-1)  # (batch_size, timesteps_decoder, timesteps_input)\n",
        "\n",
        "        # Apply softmax to calculate attention weights\n",
        "        attention_weights = self.attention_softmax(attention_scores)  # (batch_size, timesteps_decoder, timesteps_input)\n",
        "\n",
        "        return attention_weights\n",
        "\n",
        "\n",
        "\n",
        "# Parameters\n",
        "input_dim = X_train.shape[2]  # Number of features in the input sequence (e.g., 6)\n",
        "output_dim = Y_train.shape[2]  # Number of features in the output sequence (e.g., 3)\n",
        "latent_dim = 64  # Latent dimension for LSTM layers\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "timesteps_input = X_train.shape[1]  # Window size\n",
        "timesteps_output = Y_train.shape[1]  # Prediction horizon\n",
        "\n",
        "# Initialize the model\n",
        "model = Seq2SeqWithAttention(\n",
        "    input_dim=input_dim,\n",
        "    output_dim=output_dim,\n",
        "    latent_dim=latent_dim,\n",
        "    num_layers=num_layers,\n",
        "    timesteps_input=timesteps_input,\n",
        "    timesteps_output=timesteps_output\n",
        ")\n",
        "\n",
        "# Define the custom R² metric\n",
        "def r2_score(y_true, y_pred):\n",
        "    # Calculate the residual sum of squares\n",
        "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=None)\n",
        "    # Calculate the total sum of squares\n",
        "    y_true_mean = tf.reduce_mean(y_true, axis=None)\n",
        "    ss_tot = tf.reduce_sum(tf.square(y_true - y_true_mean), axis=None)\n",
        "    # Calculate R² score\n",
        "    r2 = 1 - (ss_res / (ss_tot + tf.keras.backend.epsilon()))\n",
        "    return r2\n",
        "\n",
        "\n",
        "# Compile the model with the R² metric\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[r2_score])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train, Y_train[:, :-1, :]],  # Encoder input: X_train, Decoder input: Y_train[:, :-1, :]\n",
        "    Y_train[:, 1:, :],  # Actual target sequence for training\n",
        "    epochs=15,\n",
        "    batch_size=32,\n",
        "    validation_data=([X_val, Y_val[:, :-1, :]], Y_val[:, 1:, :]),  # For validation, use the same format\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-57TvN-pOkA",
        "outputId": "c39a4d59-1ed6-4b94-c528-c1a877363cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - loss: 0.2407 - r2_score: 0.6801 - val_loss: 0.0319 - val_r2_score: 0.9643\n",
            "Epoch 2/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0239 - r2_score: 0.9733 - val_loss: 0.0229 - val_r2_score: 0.9753\n",
            "Epoch 3/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0181 - r2_score: 0.9812 - val_loss: 0.0144 - val_r2_score: 0.9853\n",
            "Epoch 4/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.0071 - r2_score: 0.9917 - val_loss: 0.0083 - val_r2_score: 0.9915\n",
            "Epoch 5/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0050 - r2_score: 0.9944 - val_loss: 0.0050 - val_r2_score: 0.9941\n",
            "Epoch 6/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0046 - r2_score: 0.9953 - val_loss: 0.0031 - val_r2_score: 0.9961\n",
            "Epoch 7/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0036 - r2_score: 0.9958 - val_loss: 0.0030 - val_r2_score: 0.9963\n",
            "Epoch 8/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0021 - r2_score: 0.9976 - val_loss: 0.0027 - val_r2_score: 0.9968\n",
            "Epoch 9/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0018 - r2_score: 0.9978 - val_loss: 0.0017 - val_r2_score: 0.9976\n",
            "Epoch 10/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0018 - r2_score: 0.9979 - val_loss: 0.0016 - val_r2_score: 0.9978\n",
            "Epoch 11/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0014 - r2_score: 0.9984 - val_loss: 0.0016 - val_r2_score: 0.9979\n",
            "Epoch 12/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0016 - r2_score: 0.9981 - val_loss: 0.0013 - val_r2_score: 0.9982\n",
            "Epoch 13/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 9.0332e-04 - r2_score: 0.9989 - val_loss: 0.0015 - val_r2_score: 0.9981\n",
            "Epoch 14/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0012 - r2_score: 0.9986 - val_loss: 9.7986e-04 - val_r2_score: 0.9986\n",
            "Epoch 15/15\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.2562e-04 - r2_score: 0.9990 - val_loss: 9.6651e-04 - val_r2_score: 0.9987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79FR-ay9YLdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model and capture loss and metrics\n",
        "test_results = model.evaluate([X_val, Y_val[:, :-1, :]], Y_val[:, 1:, :], verbose=1)\n",
        "test_loss, test_r2 = test_results\n",
        "\n",
        "# Print test loss and R² score\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test R² Score (Accuracy): {test_r2}\")"
      ],
      "metadata": {
        "id": "R2gZFfuFYLbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97ad7c7-f748-439e-a596-acaa956201e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.5378e-04 - r2_score: 0.9989\n",
            "Test Loss: 0.0009665137040428817\n",
            "Test R² Score (Accuracy): 0.9986791014671326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qAQdNSKLYLY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HelVDtZjulFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define the Seq2Seq model with Attention\n",
        "class Seq2SeqWithAttention(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, latent_dim, num_layers, timesteps_input, timesteps_output):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.timesteps_input = timesteps_input\n",
        "        self.timesteps_output = timesteps_output\n",
        "\n",
        "        self.encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "        self.decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "\n",
        "        self.attention_dense = tf.keras.layers.Dense(1)\n",
        "        self.attention_softmax = tf.keras.layers.Softmax(axis=1)\n",
        "\n",
        "        self.output_dense = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "\n",
        "        # Encoder\n",
        "        encoder_outputs, state_h, state_c = self.encoder_lstm(encoder_inputs)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_outputs, _, _ = self.decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = self.attention_score(encoder_outputs, decoder_outputs)\n",
        "\n",
        "        # Compute context vector\n",
        "        attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)\n",
        "        encoder_outputs_expanded = tf.expand_dims(encoder_outputs, axis=1)\n",
        "        context_vector = tf.reduce_sum(encoder_outputs_expanded * attention_weights_expanded, axis=2)\n",
        "\n",
        "        # Combine context vector and decoder outputs\n",
        "        decoder_combined_context = tf.concat([context_vector, decoder_outputs], axis=-1)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = self.output_dense(decoder_combined_context)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def attention_score(self, encoder_outputs, decoder_outputs):\n",
        "        timesteps_decoder = tf.shape(decoder_outputs)[1]\n",
        "\n",
        "        # Tile decoder outputs to match the encoder sequence length\n",
        "        decoder_expanded = tf.expand_dims(decoder_outputs, axis=2)\n",
        "        decoder_tiled = tf.tile(decoder_expanded, [1, 1, self.timesteps_input, 1])\n",
        "\n",
        "        # Expand encoder outputs to match decoder timesteps\n",
        "        encoder_expanded = tf.expand_dims(encoder_outputs, axis=1)\n",
        "        encoder_tiled = tf.tile(encoder_expanded, [1, timesteps_decoder, 1, 1])\n",
        "\n",
        "        # Concatenate encoder and decoder outputs\n",
        "        concat = tf.concat([encoder_tiled, decoder_tiled], axis=-1)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = self.attention_dense(concat)\n",
        "        attention_scores = tf.squeeze(attention_scores, axis=-1)\n",
        "\n",
        "        # Apply softmax to calculate attention weights\n",
        "        attention_weights = self.attention_softmax(attention_scores)\n",
        "\n",
        "        return attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"input_dim\": self.output_dense.units,\n",
        "            \"output_dim\": self.timesteps_output,\n",
        "            \"latent_dim\": self.encoder_lstm.units,\n",
        "            \"num_layers\": 1,  # Modify as needed\n",
        "            \"timesteps_input\": self.timesteps_input,\n",
        "            \"timesteps_output\": self.timesteps_output,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "# Define the custom R² metric\n",
        "def r2_score(y_true, y_pred):\n",
        "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=None)\n",
        "    y_true_mean = tf.reduce_mean(y_true, axis=None)\n",
        "    ss_tot = tf.reduce_sum(tf.square(y_true - y_true_mean), axis=None)\n",
        "    r2 = 1 - (ss_res / (ss_tot + tf.keras.backend.epsilon()))\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "6uYP3aVDulCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parameters\n",
        "input_dim = X_train.shape[2]\n",
        "\n",
        "# Ensure output_dim matches the target data\n",
        "output_dim = Y_train.shape[2]\n",
        "\n",
        "# Initialize the model\n",
        "model = Seq2SeqWithAttention(\n",
        "    input_dim=X_train.shape[2],\n",
        "    output_dim=output_dim,\n",
        "    latent_dim=latent_dim,\n",
        "    num_layers=num_layers,\n",
        "    timesteps_input=X_train.shape[1],\n",
        "    timesteps_output=Y_train.shape[1]\n",
        ")\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[r2_score])\n",
        "model.fit([X_train, Y_train[:, :-1, :]], Y_train[:, 1:, :], epochs=5, batch_size=32,\n",
        "          validation_data=([X_val, Y_val[:, :-1, :]], Y_val[:, 1:, :]), verbose=1)\n",
        "\n",
        "# Save the model\n",
        "# model.load_weights(\"/content/seq2seq_with_attention_model.h5\")\n",
        "# Save the model including both architecture and weights\n",
        "model.save(\"./seq2seq_with_attention_model.h5\", save_format=\"tf\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QauqH5ZFulAX",
        "outputId": "23b111f1-1934-45e4-a3fd-429d10ef87ec"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - loss: 0.3066 - r2_score: 0.6396 - val_loss: 0.0330 - val_r2_score: 0.9630\n",
            "Epoch 2/5\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0237 - r2_score: 0.9731 - val_loss: 0.0200 - val_r2_score: 0.9783\n",
            "Epoch 3/5\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0132 - r2_score: 0.9839 - val_loss: 0.0122 - val_r2_score: 0.9863\n",
            "Epoch 4/5\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0079 - r2_score: 0.9893 - val_loss: 0.0067 - val_r2_score: 0.9924\n",
            "Epoch 5/5\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0055 - r2_score: 0.9940 - val_loss: 0.0040 - val_r2_score: 0.9953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=tf\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mcla3Dtcuk8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_input_data(df, window_size, continuous_features, scaler):\n",
        "    \"\"\"\n",
        "    Preprocess the input data for prediction based on the last `window_size` time steps.\n",
        "    \"\"\"\n",
        "    # Scale the continuous features for the input\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[continuous_features] = scaler.transform(df[continuous_features])\n",
        "\n",
        "    # Get the last `window_size` steps of data\n",
        "    X_seq = df_scaled[continuous_features].values[-window_size:]\n",
        "\n",
        "    # Reshape to (1, window_size, features) for model input\n",
        "    X_seq = np.expand_dims(X_seq, axis=0)  # Shape becomes (1, window_size, len(continuous_features))\n",
        "\n",
        "    return X_seq\n",
        "\n",
        "def predict_sequence(model, df, window_size, continuous_features, scaler):\n",
        "    \"\"\"\n",
        "    Predict the next `prediction_horizon` time steps using the trained Seq2Seq model.\n",
        "    \"\"\"\n",
        "    # Preprocess the input data for prediction\n",
        "    X_seq = preprocess_input_data(df, window_size, continuous_features, scaler)\n",
        "\n",
        "    # Generate predictions using the model\n",
        "    predictions = model.predict([X_seq, X_seq])  # Here, we provide both encoder and decoder inputs\n",
        "\n",
        "    # The output shape should be (1, prediction_horizon, features)\n",
        "    predicted_values = predictions[0]  # Get the prediction for the first batch\n",
        "\n",
        "    return predicted_values\n",
        "\n",
        "# Example Usage:\n",
        "\n",
        "# Define parameters\n",
        "window_size = 10  # Number of observed states\n",
        "prediction_horizon = 5  # Number of time steps to predict\n",
        "continuous_features = ['LAT', 'LON', 'SOG']\n",
        "\n",
        "# Assuming your `ais_data` DataFrame contains the historical data and is already preprocessed\n",
        "# Fit the scaler on the historical data (this should be done once during training)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the continuous features of the entire dataset or the training dataset\n",
        "scaler.fit(ais_data[continuous_features])\n",
        "\n",
        "# Prepare data for the last available timestamp\n",
        "last_data_point = ais_data.iloc[-1:]\n",
        "\n",
        "# Make a prediction for the next `prediction_horizon` time steps\n",
        "predicted_values = predict_sequence(model, ais_data, window_size, continuous_features, scaler)\n",
        "\n",
        "# Output the predicted values\n",
        "predicted_df = pd.DataFrame(predicted_values, columns=continuous_features)\n",
        "print(predicted_df)\n"
      ],
      "metadata": {
        "id": "TXXlDC8zuk1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4778b47-141f-4ba9-a696-0486087ad26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 991ms/step\n",
            "        LAT       LON       SOG\n",
            "0 -0.314065  0.148184  0.035697\n",
            "1 -0.323168  0.152857  0.177661\n",
            "2 -0.325256  0.155183  0.350927\n",
            "3 -0.325709  0.156682  0.410154\n",
            "4 -0.326120  0.157343  0.363400\n",
            "5 -0.326072  0.157043  0.243034\n",
            "6 -0.325038  0.155837  0.088761\n",
            "7 -0.323136  0.154071 -0.068715\n",
            "8 -0.320919  0.152154 -0.213066\n",
            "9 -0.318966  0.150375 -0.339225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b17e42_gukzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hof-Y4DeRnfm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define the Seq2Seq model with Attention\n",
        "class Seq2SeqWithAttention(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, latent_dim, num_layers, timesteps_input, timesteps_output):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.timesteps_input = timesteps_input\n",
        "        self.timesteps_output = timesteps_output\n",
        "\n",
        "        self.encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "        self.decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "\n",
        "        self.attention_dense = tf.keras.layers.Dense(1)\n",
        "        self.attention_softmax = tf.keras.layers.Softmax(axis=1)\n",
        "\n",
        "        self.output_dense = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "\n",
        "        # Encoder\n",
        "        encoder_outputs, state_h, state_c = self.encoder_lstm(encoder_inputs)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_outputs, _, _ = self.decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = self.attention_score(encoder_outputs, decoder_outputs)\n",
        "\n",
        "        # Compute context vector\n",
        "        attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)\n",
        "        encoder_outputs_expanded = tf.expand_dims(encoder_outputs, axis=1)\n",
        "        context_vector = tf.reduce_sum(encoder_outputs_expanded * attention_weights_expanded, axis=2)\n",
        "\n",
        "        # Combine context vector and decoder outputs\n",
        "        decoder_combined_context = tf.concat([context_vector, decoder_outputs], axis=-1)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = self.output_dense(decoder_combined_context)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def attention_score(self, encoder_outputs, decoder_outputs):\n",
        "        timesteps_decoder = tf.shape(decoder_outputs)[1]\n",
        "\n",
        "        # Tile decoder outputs to match the encoder sequence length\n",
        "        decoder_expanded = tf.expand_dims(decoder_outputs, axis=2)\n",
        "        decoder_tiled = tf.tile(decoder_expanded, [1, 1, self.timesteps_input, 1])\n",
        "\n",
        "        # Expand encoder outputs to match decoder timesteps\n",
        "        encoder_expanded = tf.expand_dims(encoder_outputs, axis=1)\n",
        "        encoder_tiled = tf.tile(encoder_expanded, [1, timesteps_decoder, 1, 1])\n",
        "\n",
        "        # Concatenate encoder and decoder outputs\n",
        "        concat = tf.concat([encoder_tiled, decoder_tiled], axis=-1)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = self.attention_dense(concat)\n",
        "        attention_scores = tf.squeeze(attention_scores, axis=-1)\n",
        "\n",
        "        # Apply softmax to calculate attention weights\n",
        "        attention_weights = self.attention_softmax(attention_scores)\n",
        "\n",
        "        return attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"input_dim\": self.output_dense.units,\n",
        "            \"output_dim\": self.timesteps_output,\n",
        "            \"latent_dim\": self.encoder_lstm.units,\n",
        "            \"num_layers\": 1,  # Modify as needed\n",
        "            \"timesteps_input\": self.timesteps_input,\n",
        "            \"timesteps_output\": self.timesteps_output,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "# Define the custom R² metric\n",
        "def r2_score(y_true, y_pred):\n",
        "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=None)\n",
        "    y_true_mean = tf.reduce_mean(y_true, axis=None)\n",
        "    ss_tot = tf.reduce_sum(tf.square(y_true - y_true_mean), axis=None)\n",
        "    r2 = 1 - (ss_res / (ss_tot + tf.keras.backend.epsilon()))\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "u1nAnjnxRnc2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parameters\n",
        "input_dim = X_train.shape[2]\n",
        "\n",
        "# Ensure output_dim matches the target data\n",
        "output_dim = Y_train.shape[2]\n",
        "\n",
        "# Initialize the model\n",
        "model = Seq2SeqWithAttention(\n",
        "    input_dim=X_train.shape[2],\n",
        "    output_dim=output_dim,\n",
        "    latent_dim=latent_dim,\n",
        "    num_layers=num_layers,\n",
        "    timesteps_input=X_train.shape[1],\n",
        "    timesteps_output=Y_train.shape[1]\n",
        ")\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[r2_score])\n",
        "model.fit([X_train, Y_train[:, :-1, :]], Y_train[:, 1:, :], epochs=2, batch_size=32,\n",
        "          validation_data=([X_val, Y_val[:, :-1, :]], Y_val[:, 1:, :]), verbose=1)\n",
        "\n",
        "model.save_weights(\"seq2seq_with_attention.weights.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lqoa1t2Rnax",
        "outputId": "f02a7fea-f069-4ea8-c7a8-51eda209f205"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 0.3729 - r2_score: 0.6282 - val_loss: 0.0383 - val_r2_score: 0.9607\n",
            "Epoch 2/2\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0202 - r2_score: 0.9759 - val_loss: 0.0207 - val_r2_score: 0.9780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinitialize the model with the same architecture\n",
        "model = Seq2SeqWithAttention(\n",
        "    input_dim=X_train.shape[2],\n",
        "    output_dim=output_dim,\n",
        "    latent_dim=latent_dim,\n",
        "    num_layers=num_layers,\n",
        "    timesteps_input=X_train.shape[1],\n",
        "    timesteps_output=Y_train.shape[1]\n",
        ")\n",
        "\n",
        "# Compile the model (same optimizer and loss function used during training)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[r2_score])\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_weights(\"seq2seq_with_attention.weights.h5\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkKbL2G5RnWi",
        "outputId": "9b8fc095-82d9-4c79-c07f-20e3508232a7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0Wp4YpmRnUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_input_data(df, window_size, continuous_features, scaler):\n",
        "    \"\"\"\n",
        "    Preprocess the input data for prediction based on the last `window_size` time steps.\n",
        "    \"\"\"\n",
        "    # Scale the continuous features for the input\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[continuous_features] = scaler.transform(df[continuous_features])\n",
        "\n",
        "    # Get the last `window_size` steps of data\n",
        "    X_seq = df_scaled[continuous_features].values[-window_size:]\n",
        "\n",
        "    # Reshape to (1, window_size, features) for model input\n",
        "    X_seq = np.expand_dims(X_seq, axis=0)  # Shape becomes (1, window_size, len(continuous_features))\n",
        "\n",
        "    return X_seq\n",
        "\n",
        "def predict_sequence(model, df, window_size, continuous_features, scaler, weights_path):\n",
        "    \"\"\"\n",
        "    Predict the next `prediction_horizon` time steps using the trained Seq2Seq model.\n",
        "    The model weights are loaded from the specified weights file.\n",
        "    \"\"\"\n",
        "    # Reinitialize the model (this should match the architecture used during training)\n",
        "    model = Seq2SeqWithAttention(\n",
        "        input_dim=df[continuous_features].shape[1],  # Number of features in your input\n",
        "        output_dim=3,\n",
        "        latent_dim=128,  # Modify based on  model's latent dimension\n",
        "        num_layers=2,  # Modify based on  model's layers\n",
        "        timesteps_input=window_size,\n",
        "        timesteps_output=prediction_horizon\n",
        "    )\n",
        "\n",
        "    # Compile the model before loading weights (same as during training)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    # Load the saved model weights\n",
        "    model.load_weights(weights_path)\n",
        "\n",
        "    # Preprocess the input data for prediction\n",
        "    X_seq = preprocess_input_data(df, window_size, continuous_features, scaler)\n",
        "\n",
        "    # Generate predictions using the model\n",
        "    predictions = model.predict([X_seq, X_seq])  # provide both encoder and decoder inputs\n",
        "\n",
        "    # The output shape should be (1, prediction_horizon, features), e.g. (1, 5, 3) if predicting 3 features\n",
        "    predicted_values = predictions[0]  # Get the prediction for the first batch\n",
        "\n",
        "    return predicted_values\n",
        "\n",
        "\n",
        "window_size = 10  # Number of observed states\n",
        "prediction_horizon = 5  # Number of time steps to predict\n",
        "continuous_features = ['LAT', 'LON', 'SOG']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(ais_data[continuous_features])\n",
        "weights_path = '/content/seq2seq_with_attention.weights.h5'\n",
        "last_data_point = ais_data.iloc[-1:]\n",
        "predicted_values = predict_sequence(model=None, df=ais_data, window_size=window_size,\n",
        "                                    continuous_features=continuous_features, scaler=scaler,\n",
        "                                    weights_path=weights_path)\n",
        "predicted_df = pd.DataFrame(predicted_values, columns=continuous_features)\n",
        "print(predicted_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js06lYrmjJHN",
        "outputId": "09371343-c1ec-4e40-a2c2-aac1a73bfdb3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "        LAT       LON       SOG\n",
            "0  0.062287 -0.026731  0.039613\n",
            "1  0.062649 -0.019276  0.029238\n",
            "2  0.063329 -0.009782  0.021983\n",
            "3  0.063997 -0.001607  0.014890\n",
            "4  0.064392  0.004644  0.008200\n",
            "5  0.064430  0.008967  0.001949\n",
            "6  0.064109  0.011505 -0.003882\n",
            "7  0.063461  0.012467 -0.009339\n",
            "8  0.062537  0.012075 -0.014474\n",
            "9  0.061387  0.010544 -0.019344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pGp3xVHgjJEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_u3pTZK9jJCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input_data_with_time(df, window_size, continuous_features, scaler):\n",
        "    \"\"\"\n",
        "    Preprocess the input data for prediction based on the last `window_size` time steps,\n",
        "    including time as a feature.\n",
        "    \"\"\"\n",
        "    # Scale the continuous features\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[continuous_features] = scaler.transform(df[continuous_features])\n",
        "\n",
        "    # Convert timestamp to numeric values (for example, to the number of seconds)\n",
        "    df_scaled['BaseDateTime'] = (df_scaled['BaseDateTime'] - df_scaled['BaseDateTime'].min()).dt.total_seconds()\n",
        "\n",
        "    # Get the last `window_size` steps of data, including time\n",
        "    X_seq = df_scaled[continuous_features + ['BaseDateTime']].values[-window_size:]\n",
        "\n",
        "    # Reshape to (1, window_size, features) for model input\n",
        "    X_seq = np.expand_dims(X_seq, axis=0)  # Shape becomes (1, window_size, len(continuous_features) + 1)\n",
        "\n",
        "    return X_seq\n",
        "\n",
        "def predict_sequence_with_time(model, df, window_size, continuous_features, scaler, weights_path, prediction_horizon):\n",
        "    \"\"\"\n",
        "    Predict the next `prediction_horizon` time steps and their corresponding timestamps using the trained Seq2Seq model.\n",
        "    \"\"\"\n",
        "    # Reinitialize and compile the model (ensure the architecture matches)\n",
        "    model = Seq2SeqWithAttention(\n",
        "        input_dim=df[continuous_features].shape[1] + 1,  # Number of features + 1 for time\n",
        "        output_dim=3 + 1,  # 3 features (LAT, LON, SOG) + 1 for predicted time\n",
        "        latent_dim=128,  # Latent dimension size for the LSTM layers\n",
        "        num_layers=2,  # Number of LSTM layers\n",
        "        timesteps_input=window_size,\n",
        "        timesteps_output=prediction_horizon\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    # Load the saved weights (the pre-trained model)\n",
        "    model.load_weights(weights_path)\n",
        "\n",
        "    # Preprocess the input data (get last `window_size` steps)\n",
        "    X_seq = preprocess_input_data_with_time(df, window_size, continuous_features, scaler)\n",
        "\n",
        "    # Make predictions (use both encoder and decoder inputs)\n",
        "    predictions = model.predict([X_seq, X_seq])  # Here, we provide both encoder and decoder inputs\n",
        "\n",
        "    # Extract the predicted values for the batch\n",
        "    predicted_values = predictions[0]  # Get predictions for the first batch\n",
        "\n",
        "    # Extract only the predicted values for the next `prediction_horizon` steps\n",
        "    predicted_values = predicted_values[-prediction_horizon:]  # Slice only the predicted part\n",
        "\n",
        "    # Convert time predictions back to actual timestamps\n",
        "    last_timestamp = df['BaseDateTime'].iloc[-1]  # Get the last known timestamp\n",
        "\n",
        "    # Convert predicted time delta (in seconds) to a Timedelta object\n",
        "    predicted_timestamps = pd.to_timedelta(predicted_values[:, -1], unit='s') + pd.to_datetime(last_timestamp)\n",
        "\n",
        "    # Extract predicted features (LAT, LON, SOG)\n",
        "    predicted_features = predicted_values[:, :-1]\n",
        "\n",
        "    # Convert the predictions into a DataFrame\n",
        "    predicted_df = pd.DataFrame(predicted_features, columns=continuous_features)\n",
        "    predicted_df['BaseDateTime'] = predicted_timestamps\n",
        "\n",
        "    # Get the observations (the last `window_size` steps from the original data)\n",
        "    observed_df = df.iloc[-window_size:].copy()\n",
        "\n",
        "    # Prepare observed DataFrame (same features, scaled values)\n",
        "    observed_df[continuous_features] = scaler.transform(observed_df[continuous_features])\n",
        "\n",
        "    # Convert the observed DataFrame into a format that includes time\n",
        "    observed_df['BaseDateTime'] = pd.to_datetime(observed_df['BaseDateTime'])\n",
        "\n",
        "    # Return both observed and predicted DataFrames\n",
        "    return observed_df, predicted_df\n",
        "\n",
        "# Example Usage:\n",
        "\n",
        "window_size = 10  # Number of observed states\n",
        "prediction_horizon = 5  # Number of time steps to predict\n",
        "continuous_features = ['LAT', 'LON', 'SOG']\n",
        "\n",
        "# Assume the `ais_data` DataFrame is already loaded with data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(ais_data[continuous_features])\n",
        "\n",
        "weights_path = '/content/seq2seq_with_attention.weights.h5'\n",
        "\n",
        "# Predict the next `prediction_horizon` time steps\n",
        "observed_df, predicted_df = predict_sequence_with_time(\n",
        "    model=None,\n",
        "    df=ais_data,\n",
        "    window_size=window_size,\n",
        "    continuous_features=continuous_features,\n",
        "    scaler=scaler,\n",
        "    weights_path=weights_path,\n",
        "    prediction_horizon=prediction_horizon  # Pass prediction_horizon here\n",
        ")\n",
        "\n",
        "# Print the observed data and predicted data separately\n",
        "print(\"Observed Data:\")\n",
        "print(observed_df)\n",
        "\n",
        "print(\"\\nPredicted Data:\")\n",
        "print(predicted_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWxIbh8vjJAI",
        "outputId": "aedcf629-ec19-4e99-c783-8ee400edcfbf"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Observed Data:\n",
            "             BaseDateTime       LAT       LON       MMSI PatternDescriptor  \\\n",
            "23770 2022-03-31 00:00:52 -0.495197  0.256379  985346207        Stationary   \n",
            "23771 2022-03-31 00:01:02 -0.495197  0.256378  985346207        Stationary   \n",
            "23772 2022-03-31 00:01:12 -0.495196  0.256378  985346207        Stationary   \n",
            "23773 2022-03-31 00:01:22 -0.495196  0.256378  985346207        Stationary   \n",
            "23774 2022-03-31 00:01:32 -0.495196  0.256378  985346207        Stationary   \n",
            "23775 2022-03-31 00:01:42 -0.495196  0.256378  985346207        Stationary   \n",
            "23776 2022-03-31 00:01:52 -0.495196  0.256378  985346207        Stationary   \n",
            "23777 2022-03-31 00:02:02 -0.495195  0.256378  985346207        Stationary   \n",
            "23778 2022-03-31 00:02:12 -0.495195  0.256378  985346207        Stationary   \n",
            "23779 2022-03-31 00:02:22 -0.495195  0.256378  985346207        Stationary   \n",
            "\n",
            "       Pattern_High Speed  Pattern_Slow Movement  Pattern_Stationary  \\\n",
            "23770                 0.0                    0.0                 1.0   \n",
            "23771                 0.0                    0.0                 1.0   \n",
            "23772                 0.0                    0.0                 1.0   \n",
            "23773                 0.0                    0.0                 1.0   \n",
            "23774                 0.0                    0.0                 1.0   \n",
            "23775                 0.0                    0.0                 1.0   \n",
            "23776                 0.0                    0.0                 1.0   \n",
            "23777                 0.0                    0.0                 1.0   \n",
            "23778                 0.0                    0.0                 1.0   \n",
            "23779                 0.0                    0.0                 1.0   \n",
            "\n",
            "            SOG  SegmentID  \n",
            "23770  0.155949       2377  \n",
            "23771  0.250956       2377  \n",
            "23772  0.345963       2377  \n",
            "23773  0.250956       2377  \n",
            "23774  0.155949       2377  \n",
            "23775  0.060942       2377  \n",
            "23776 -0.034065       2377  \n",
            "23777 -0.129072       2377  \n",
            "23778 -0.224079       2377  \n",
            "23779 -0.319086       2377  \n",
            "\n",
            "Predicted Data:\n",
            "        LAT       LON       SOG                  BaseDateTime\n",
            "0 -0.587018 -0.020647  0.421970 2022-03-31 00:02:21.366570592\n",
            "1 -0.560196 -0.050958  0.383290 2022-03-31 00:02:21.479138971\n",
            "2 -0.470193 -0.075909  0.320281 2022-03-31 00:02:21.737460256\n",
            "3 -0.418520 -0.056782  0.333100 2022-03-31 00:02:21.936796069\n",
            "4 -0.406599 -0.046028  0.339336 2022-03-31 00:02:21.996936500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNSESIMSjI7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version -2"
      ],
      "metadata": {
        "id": "Cqh1mSV_pMfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Enhancements:\n",
        "\n",
        "    Time Handling: Time should be treated more explicitly as a separate feature. You may want to include the BaseDateTime as a feature during training and use it properly during prediction to handle time deltas.\n",
        "\n",
        "    Separate Time Prediction Output: As we already have a separate layer for predicting the time delta (self.time_dense), we need to ensure the output from this layer is used and treated properly for time-based predictions.\n",
        "\n",
        "    Investigating Time Prediction: We should output the predicted time deltas (in seconds) and then investigate how accurately the model is predicting these time deltas. The model will predict the change in time (time delta) between the last timestamp in the input sequence and the predicted timestamps.\n",
        "\n",
        "Step-by-Step Implementation\n",
        "1. Time Handling in the Model\n",
        "\n",
        "To handle time effectively, the model will use time as an input feature (possibly encoded as a number or time-related feature). Additionally, we will focus on making time predictions explicit by having the model predict the time deltas separately. These time deltas will be added to the last timestamp to generate future timestamps.\n",
        "2. Separate Time Prediction Output\n",
        "\n",
        "We already have the time_dense layer for time prediction, which predicts the time delta. We'll make sure that this output is handled properly.\n",
        "3. Investigating Time Prediction\n",
        "\n",
        "We will add a way to calculate the difference between predicted timestamps and the actual time deltas. We can visualize this or calculate metrics like RMSE or MAE to evaluate the time predictions."
      ],
      "metadata": {
        "id": "GDwWqkJ4q-Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define the Seq2Seq model with Attention and separate time prediction\n",
        "class Seq2SeqWithAttentionAndTimePrediction(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, latent_dim, num_layers, timesteps_input, timesteps_output):\n",
        "        super(Seq2SeqWithAttentionAndTimePrediction, self).__init__()\n",
        "        self.timesteps_input = timesteps_input\n",
        "        self.timesteps_output = timesteps_output\n",
        "\n",
        "        self.encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "        self.decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "\n",
        "        self.attention_dense = tf.keras.layers.Dense(1)\n",
        "        self.attention_softmax = tf.keras.layers.Softmax(axis=1)\n",
        "\n",
        "        # Output layers for feature predictions (LAT, LON, SOG)\n",
        "        self.output_dense = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "        # Separate layer for time delta prediction (time in seconds)\n",
        "        self.time_dense = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "\n",
        "        # Encoder\n",
        "        encoder_outputs, state_h, state_c = self.encoder_lstm(encoder_inputs)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_outputs, _, _ = self.decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = self.attention_score(encoder_outputs, decoder_outputs)\n",
        "\n",
        "        # Compute context vector\n",
        "        attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)\n",
        "        encoder_outputs_expanded = tf.expand_dims(encoder_outputs, axis=1)\n",
        "        context_vector = tf.reduce_sum(encoder_outputs_expanded * attention_weights_expanded, axis=2)\n",
        "\n",
        "        # Combine context vector and decoder outputs\n",
        "        decoder_combined_context = tf.concat([context_vector, decoder_outputs], axis=-1)\n",
        "\n",
        "        # Output layer for features (LAT, LON, SOG)\n",
        "        outputs = self.output_dense(decoder_combined_context)\n",
        "\n",
        "        # Separate time prediction (time delta in seconds)\n",
        "        time_delta = self.time_dense(decoder_combined_context)  # Predict time delta (in seconds)\n",
        "\n",
        "        return outputs, time_delta\n",
        "\n",
        "    def attention_score(self, encoder_outputs, decoder_outputs):\n",
        "        timesteps_decoder = tf.shape(decoder_outputs)[1]\n",
        "\n",
        "        # Tile decoder outputs to match the encoder sequence length\n",
        "        decoder_expanded = tf.expand_dims(decoder_outputs, axis=2)\n",
        "        decoder_tiled = tf.tile(decoder_expanded, [1, 1, self.timesteps_input, 1])\n",
        "\n",
        "        # Expand encoder outputs to match decoder timesteps\n",
        "        encoder_expanded = tf.expand_dims(encoder_outputs, axis=1)\n",
        "        encoder_tiled = tf.tile(encoder_expanded, [1, timesteps_decoder, 1, 1])\n",
        "\n",
        "        # Concatenate encoder and decoder outputs\n",
        "        concat = tf.concat([encoder_tiled, decoder_tiled], axis=-1)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = self.attention_dense(concat)\n",
        "        attention_scores = tf.squeeze(attention_scores, axis=-1)\n",
        "\n",
        "        # Apply softmax to calculate attention weights\n",
        "        attention_weights = self.attention_softmax(attention_scores)\n",
        "\n",
        "        return attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"input_dim\": self.output_dense.units,\n",
        "            \"output_dim\": self.timesteps_output,\n",
        "            \"latent_dim\": self.encoder_lstm.units,\n",
        "            \"num_layers\": 1,  # Modify as needed\n",
        "            \"timesteps_input\": self.timesteps_input,\n",
        "            \"timesteps_output\": self.timesteps_output,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Define the custom R² metric\n",
        "def r2_score(y_true, y_pred):\n",
        "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=None)\n",
        "    y_true_mean = tf.reduce_mean(y_true, axis=None)\n",
        "    ss_tot = tf.reduce_sum(tf.square(y_true - y_true_mean), axis=None)\n",
        "    r2 = 1 - (ss_res / (ss_tot + tf.keras.backend.epsilon()))\n",
        "    return r2\n",
        "\n",
        "# Compile the model with separate metrics for both outputs\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',  # Mean Squared Error for both outputs\n",
        "    metrics=[\n",
        "        r2_score,        # For the feature prediction output (LAT, LON, SOG)\n",
        "        'mae'            # For the time delta prediction output (time delta in seconds)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    [X_train, Y_train[:, :-1, :]],  # Decoder input\n",
        "    [Y_train[:, 1:, :], Y_train[:, 1:, -1:]],  # The two outputs: features and time delta\n",
        "    epochs=2,\n",
        "    batch_size=32,\n",
        "    validation_data=([X_val, Y_val[:, :-1, :]], [Y_val[:, 1:, :], Y_val[:, 1:, -1:]]),  # Validation data\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the model weights\n",
        "model.save_weights(\"seq2seq_with_attention_v2.weights.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9je0NyLIk05G",
        "outputId": "8840e371-388d-4631-b830-eedd8353d1fe"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 44ms/step - loss: 0.3976 - mae: 0.1819 - mse_loss: 0.1553 - r2_score: 0.6704 - val_loss: 0.1128 - val_mae: 0.1052 - val_mse_loss: 0.0717 - val_r2_score: 0.9555\n",
            "Epoch 2/2\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0658 - mae: 0.0812 - mse_loss: 0.0416 - r2_score: 0.9708 - val_loss: 0.0565 - val_mae: 0.0693 - val_mse_loss: 0.0329 - val_r2_score: 0.9731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qV8z5XCTk0qT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess input data by extracting a window of time series and scaling the continuous features\n",
        "def preprocess_input_data_with_time(df, window_size, continuous_features, scaler):\n",
        "    \"\"\"\n",
        "    Preprocess the input data for prediction based on the last `window_size` time steps,\n",
        "    including time as a feature.\n",
        "    \"\"\"\n",
        "    # Scale the continuous features\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[continuous_features] = scaler.transform(df[continuous_features])\n",
        "\n",
        "    # Convert timestamp to numeric values (for example, to the number of seconds)\n",
        "    df_scaled['BaseDateTime'] = (df_scaled['BaseDateTime'] - df_scaled['BaseDateTime'].min()).dt.total_seconds()\n",
        "\n",
        "    # Get the last `window_size` steps of data, including time\n",
        "    X_seq = df_scaled[continuous_features + ['BaseDateTime']].values[-window_size:]\n",
        "\n",
        "    # Reshape to (1, window_size, features) for model input\n",
        "    X_seq = np.expand_dims(X_seq, axis=0)  # Shape becomes (1, window_size, len(continuous_features) + 1)\n",
        "\n",
        "    return X_seq\n",
        "\n",
        "\n",
        "# Prediction function\n",
        "def predict_sequence_with_time(model, df, window_size, continuous_features, scaler, weights_path, prediction_horizon):\n",
        "    \"\"\"\n",
        "    Predict the next `prediction_horizon` time steps and their corresponding timestamps using the trained Seq2Seq model.\n",
        "    \"\"\"\n",
        "    # Reinitialize and compile the model (ensure the architecture matches)\n",
        "    model = Seq2SeqWithAttentionAndTimePrediction(\n",
        "        input_dim=df[continuous_features].shape[1] + 1,  # Number of features + 1 for time\n",
        "        output_dim=3 + 1,  # 3 features (LAT, LON, SOG) + 1 for predicted time\n",
        "        latent_dim=128,  # Latent dimension size for the LSTM layers\n",
        "        num_layers=2,  # Number of LSTM layers\n",
        "        timesteps_input=window_size,\n",
        "        timesteps_output=prediction_horizon\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    # Load the saved weights (the pre-trained model)\n",
        "    model.load_weights(weights_path)\n",
        "\n",
        "    # Preprocess the input data (get last `window_size` steps)\n",
        "    X_seq = preprocess_input_data_with_time(df, window_size, continuous_features, scaler)\n",
        "\n",
        "    # Make predictions (use both encoder and decoder inputs)\n",
        "    predictions = model.predict([X_seq, X_seq])  # Here, we provide both encoder and decoder inputs\n",
        "\n",
        "    # Extract the predicted values for the batch\n",
        "    predicted_values = predictions[0]  # Get predictions for the first batch\n",
        "\n",
        "    # Slice only the predicted part based on prediction_horizon\n",
        "    predicted_values = predicted_values[0, :prediction_horizon, :]  # Take first `prediction_horizon` steps\n",
        "    print(\"Predicted Values Shape:\", predicted_values.shape)\n",
        "\n",
        "    # Extract time delta (last column) and reshape\n",
        "    time_deltas = predicted_values[:, -1]  # Time delta is in the last column\n",
        "    print(\"Extracted Time Deltas:\", time_deltas)\n",
        "    print(\"Shape of Time Deltas:\", time_deltas.shape)\n",
        "\n",
        "    # Ensure it's a 1D array of numeric values\n",
        "    time_deltas = np.squeeze(time_deltas)  # Remove any extra dimensions\n",
        "    print(\"Squeezed Time Deltas:\", time_deltas)\n",
        "    print(\"Shape of Squeezed Time Deltas:\", time_deltas.shape)\n",
        "\n",
        "    # Convert time deltas to timedelta and add them to the last timestamp\n",
        "    last_timestamp = pd.to_datetime(df['BaseDateTime'].iloc[-1])  # Last known timestamp\n",
        "    predicted_timestamps = pd.to_timedelta(time_deltas, unit='s') + last_timestamp  # Convert to actual timestamps\n",
        "\n",
        "    # Extract predicted features (LAT, LON, SOG) - Exclude the last column (time delta)\n",
        "    predicted_features = predicted_values[:, :-1]  # Exclude the last column (time delta)\n",
        "    print(\"Shape of Predicted Features (LAT, LON, SOG):\", predicted_features.shape)\n",
        "\n",
        "    # Ensure predicted_features is 2D (shape: (prediction_horizon, 3))\n",
        "    # Reshape the predicted features if necessary\n",
        "    assert predicted_features.shape[0] == len(predicted_timestamps), \"Mismatch between predicted features and timestamps length.\"\n",
        "\n",
        "    # Convert the predictions into a DataFrame\n",
        "    predicted_df = pd.DataFrame(predicted_features, columns=continuous_features)\n",
        "    predicted_df['BaseDateTime'] = predicted_timestamps\n",
        "\n",
        "    # Print the final prediction DataFrame\n",
        "    print(\"\\nPredicted Data:\")\n",
        "    print(predicted_df)\n",
        "\n",
        "    # Get the observations (the last `window_size` steps from the original data)\n",
        "    observed_df = df.iloc[-window_size:].copy()\n",
        "\n",
        "    # Prepare observed DataFrame (same features, scaled values)\n",
        "    observed_df[continuous_features] = scaler.transform(observed_df[continuous_features])\n",
        "\n",
        "    # Convert the observed DataFrame into a format that includes time\n",
        "    observed_df['BaseDateTime'] = pd.to_datetime(observed_df['BaseDateTime'])\n",
        "\n",
        "    # Return both observed and predicted DataFrames\n",
        "    return observed_df, predicted_df\n",
        "\n",
        "\n",
        "\n",
        "window_size = 10  # Number of observed states\n",
        "prediction_horizon = 5  # Number of time steps to predict\n",
        "continuous_features = ['LAT', 'LON', 'SOG']\n",
        "\n",
        "# Assuming the `ais_data` DataFrame is already loaded with data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(ais_data[continuous_features])\n",
        "\n",
        "weights_path = '/content/seq2seq_with_attention.weights.h5'\n",
        "\n",
        "# Predict the next `prediction_horizon` time steps\n",
        "observed_df, predicted_df = predict_sequence_with_time(\n",
        "    model=None,\n",
        "    df=ais_data,\n",
        "    window_size=window_size,\n",
        "    continuous_features=continuous_features,\n",
        "    scaler=scaler,\n",
        "    weights_path=weights_path,\n",
        "    prediction_horizon=prediction_horizon  # Pass prediction_horizon here\n",
        ")\n",
        "\n",
        "# Print the observed data and predicted data separately\n",
        "print(\"Observed Data:\")\n",
        "print(observed_df)\n",
        "\n",
        "print(\"\\nPredicted Data:\")\n",
        "print(predicted_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAq30Lpek0nh",
        "outputId": "9eecd8da-2820-4385-e1b9-7840bd81e0ea"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Predicted Values Shape: (5, 4)\n",
            "Extracted Time Deltas: [0.37339684 0.6036053  0.5912262  0.6592242  0.7362376 ]\n",
            "Shape of Time Deltas: (5,)\n",
            "Squeezed Time Deltas: [0.37339684 0.6036053  0.5912262  0.6592242  0.7362376 ]\n",
            "Shape of Squeezed Time Deltas: (5,)\n",
            "Shape of Predicted Features (LAT, LON, SOG): (5, 3)\n",
            "\n",
            "Predicted Data:\n",
            "        LAT       LON       SOG                  BaseDateTime\n",
            "0 -0.401255  0.783593 -0.287319 2022-03-31 00:02:22.373396844\n",
            "1 -0.672990  0.809928 -0.268888 2022-03-31 00:02:22.603605270\n",
            "2 -0.595241  0.914851 -0.289522 2022-03-31 00:02:22.591226220\n",
            "3 -0.445280  1.017585 -0.249716 2022-03-31 00:02:22.659224212\n",
            "4 -0.327354  1.089873 -0.201365 2022-03-31 00:02:22.736237586\n",
            "Observed Data:\n",
            "             BaseDateTime       LAT       LON       MMSI PatternDescriptor  \\\n",
            "23770 2022-03-31 00:00:52 -0.495197  0.256379  985346207        Stationary   \n",
            "23771 2022-03-31 00:01:02 -0.495197  0.256378  985346207        Stationary   \n",
            "23772 2022-03-31 00:01:12 -0.495196  0.256378  985346207        Stationary   \n",
            "23773 2022-03-31 00:01:22 -0.495196  0.256378  985346207        Stationary   \n",
            "23774 2022-03-31 00:01:32 -0.495196  0.256378  985346207        Stationary   \n",
            "23775 2022-03-31 00:01:42 -0.495196  0.256378  985346207        Stationary   \n",
            "23776 2022-03-31 00:01:52 -0.495196  0.256378  985346207        Stationary   \n",
            "23777 2022-03-31 00:02:02 -0.495195  0.256378  985346207        Stationary   \n",
            "23778 2022-03-31 00:02:12 -0.495195  0.256378  985346207        Stationary   \n",
            "23779 2022-03-31 00:02:22 -0.495195  0.256378  985346207        Stationary   \n",
            "\n",
            "       Pattern_High Speed  Pattern_Slow Movement  Pattern_Stationary  \\\n",
            "23770                 0.0                    0.0                 1.0   \n",
            "23771                 0.0                    0.0                 1.0   \n",
            "23772                 0.0                    0.0                 1.0   \n",
            "23773                 0.0                    0.0                 1.0   \n",
            "23774                 0.0                    0.0                 1.0   \n",
            "23775                 0.0                    0.0                 1.0   \n",
            "23776                 0.0                    0.0                 1.0   \n",
            "23777                 0.0                    0.0                 1.0   \n",
            "23778                 0.0                    0.0                 1.0   \n",
            "23779                 0.0                    0.0                 1.0   \n",
            "\n",
            "            SOG  SegmentID  \n",
            "23770  0.155949       2377  \n",
            "23771  0.250956       2377  \n",
            "23772  0.345963       2377  \n",
            "23773  0.250956       2377  \n",
            "23774  0.155949       2377  \n",
            "23775  0.060942       2377  \n",
            "23776 -0.034065       2377  \n",
            "23777 -0.129072       2377  \n",
            "23778 -0.224079       2377  \n",
            "23779 -0.319086       2377  \n",
            "\n",
            "Predicted Data:\n",
            "        LAT       LON       SOG                  BaseDateTime\n",
            "0 -0.401255  0.783593 -0.287319 2022-03-31 00:02:22.373396844\n",
            "1 -0.672990  0.809928 -0.268888 2022-03-31 00:02:22.603605270\n",
            "2 -0.595241  0.914851 -0.289522 2022-03-31 00:02:22.591226220\n",
            "3 -0.445280  1.017585 -0.249716 2022-03-31 00:02:22.659224212\n",
            "4 -0.327354  1.089873 -0.201365 2022-03-31 00:02:22.736237586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zz0DsGp0k0gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJhSoqGwpK-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nv7f9XCgpK8B"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fzXTKT8cpK6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlXlsENIpK4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "viqcw7aupK2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aTj4E41epK0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8qZ-Vc7pKyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F6EWohlFpKvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DRlueMR_pKty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tcte9PkYpKry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4lLbAkOpKpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22X8e6NEpKnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEWrehyypKky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ywU35blMpKiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bHbeAklpKgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fk5-3iDDpKdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQfGU8dvpKa-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}