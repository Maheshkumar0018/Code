{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/segmented_trajectories.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "nzHcSSvNrWV0",
        "outputId": "56284602-6330-4f96-efc0-0989a77cc966"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          BaseDateTime       LAT        LON     MMSI PatternDescriptor  \\\n",
              "0  2022-03-31 00:00:17  26.11809 -80.148150  1056261        Stationary   \n",
              "1  2022-03-31 00:00:27  26.11809 -80.148148  1056261        Stationary   \n",
              "2  2022-03-31 00:00:37  26.11809 -80.148147  1056261        Stationary   \n",
              "3  2022-03-31 00:00:47  26.11809 -80.148145  1056261        Stationary   \n",
              "4  2022-03-31 00:00:57  26.11809 -80.148143  1056261        Stationary   \n",
              "\n",
              "   Pattern_High Speed  Pattern_Slow Movement  Pattern_Stationary       SOG  \\\n",
              "0                 NaN                    NaN                 1.0  0.100000   \n",
              "1                 NaN                    NaN                 1.0  0.083607   \n",
              "2                 NaN                    NaN                 1.0  0.067213   \n",
              "3                 NaN                    NaN                 1.0  0.050820   \n",
              "4                 NaN                    NaN                 1.0  0.034426   \n",
              "\n",
              "   SegmentID  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f2d4f55-b55b-4f53-b903-4d1f6735011b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BaseDateTime</th>\n",
              "      <th>LAT</th>\n",
              "      <th>LON</th>\n",
              "      <th>MMSI</th>\n",
              "      <th>PatternDescriptor</th>\n",
              "      <th>Pattern_High Speed</th>\n",
              "      <th>Pattern_Slow Movement</th>\n",
              "      <th>Pattern_Stationary</th>\n",
              "      <th>SOG</th>\n",
              "      <th>SegmentID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-03-31 00:00:17</td>\n",
              "      <td>26.11809</td>\n",
              "      <td>-80.148150</td>\n",
              "      <td>1056261</td>\n",
              "      <td>Stationary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-03-31 00:00:27</td>\n",
              "      <td>26.11809</td>\n",
              "      <td>-80.148148</td>\n",
              "      <td>1056261</td>\n",
              "      <td>Stationary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.083607</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-03-31 00:00:37</td>\n",
              "      <td>26.11809</td>\n",
              "      <td>-80.148147</td>\n",
              "      <td>1056261</td>\n",
              "      <td>Stationary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.067213</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-03-31 00:00:47</td>\n",
              "      <td>26.11809</td>\n",
              "      <td>-80.148145</td>\n",
              "      <td>1056261</td>\n",
              "      <td>Stationary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.050820</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-03-31 00:00:57</td>\n",
              "      <td>26.11809</td>\n",
              "      <td>-80.148143</td>\n",
              "      <td>1056261</td>\n",
              "      <td>Stationary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.034426</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f2d4f55-b55b-4f53-b903-4d1f6735011b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7f2d4f55-b55b-4f53-b903-4d1f6735011b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7f2d4f55-b55b-4f53-b903-4d1f6735011b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-736a29d5-c156-44f0-8b2a-fb2da76692af\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-736a29d5-c156-44f0-8b2a-fb2da76692af')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-736a29d5-c156-44f0-8b2a-fb2da76692af button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 23780,\n  \"fields\": [\n    {\n      \"column\": \"BaseDateTime\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"2022-03-31 00:00:51\",\n          \"2022-03-31 00:01:47\",\n          \"2022-03-31 00:02:32\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LAT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.412733079296595,\n        \"min\": 17.33234,\n        \"max\": 49.57992,\n        \"num_unique_values\": 15448,\n        \"samples\": [\n          18.894856703296703,\n          30.049167142857144,\n          29.96759855072464\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LON\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.614091526161651,\n        \"min\": -158.24993,\n        \"max\": -64.03596999999999,\n        \"num_unique_values\": 15852,\n        \"samples\": [\n          -81.62841718309859,\n          -94.89116845070424,\n          -80.7338625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MMSI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57214211,\n        \"min\": 1056261,\n        \"max\": 985346207,\n        \"num_unique_values\": 1482,\n        \"samples\": [\n          316030644,\n          367007470,\n          367706830\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PatternDescriptor\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Stationary\",\n          \"High Speed\",\n          \"Slow Movement\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pattern_High Speed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pattern_Slow Movement\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pattern_Stationary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SOG\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.527375095754698,\n        \"min\": 0.0,\n        \"max\": 102.3,\n        \"num_unique_values\": 5301,\n        \"samples\": [\n          1.246031746031746\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SegmentID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 686,\n        \"min\": 0,\n        \"max\": 2377,\n        \"num_unique_values\": 2378,\n        \"samples\": [\n          2337\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Pattern_High Speed'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj5DXAyer2xs",
        "outputId": "f120e7f9-a03f-4382-f91e-8cabdb6493be"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan,  1.])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Pattern_High Speed',\t'Pattern_Slow Movement']] = df[['Pattern_High Speed',\t'Pattern_Slow Movement']].fillna(0)"
      ],
      "metadata": {
        "id": "QFs9US2urer6"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MhGTo26repG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PydG0Fnhrem-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v2Yd8-CIrefP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # Correct import of tqdm\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def k_fold_sequence_to_sequence(df, window_size, prediction_horizon, k=5, resample_interval=10):\n",
        "    \"\"\"\n",
        "    Apply K-Fold Cross-Validation for sequence-to-sequence data preparation.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Pandas DataFrame containing AIS data with columns 'BaseDateTime', 'LAT', 'LON', 'SOG', and one-hot encoded pattern descriptors.\n",
        "    - window_size: The number of data points in each input sequence (length of observed states X_k,).\n",
        "    - prediction_horizon: The number of time steps to predict (length of target sequence Y_k,h).\n",
        "    - k: Number of folds for cross-validation.\n",
        "    - resample_interval: Resampling interval for the AIS data, in seconds (assumed to be already applied in previous steps).\n",
        "\n",
        "    Returns:\n",
        "    - folds_train: List of training data for each fold (X_train, Y_train).\n",
        "    - folds_val: List of validation data for each fold (X_val, Y_val).\n",
        "    - journey_descriptors: List of journey descriptors for each sequence.\n",
        "    \"\"\"\n",
        "    folds_train = []\n",
        "    folds_val = []\n",
        "    journey_descriptors = []\n",
        "\n",
        "    # Step 1: Prepare the K-Fold split\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    # Group by MMSI\n",
        "    grouped = df.groupby('MMSI')\n",
        "\n",
        "    # Store all sequences in a list (we will split this into k-folds)\n",
        "    all_sequences_X = []\n",
        "    all_sequences_Y = []\n",
        "    all_journey_descriptors = []\n",
        "\n",
        "    # Process each MMSI group\n",
        "    for mmsi, group in tqdm(grouped, desc=\"Processing MMSI\", unit=\"MMSI\"):  # Corrected tqdm usage\n",
        "        group = group.sort_values(by='BaseDateTime')\n",
        "        group['BaseDateTime'] = pd.to_datetime(group['BaseDateTime'])\n",
        "\n",
        "        # Check if there's enough data for the sequence-to-sequence model\n",
        "        if len(group) < window_size + prediction_horizon:\n",
        "            continue  # Skip this MMSI if it doesn't have enough data\n",
        "\n",
        "        # Step 2: One-hot encode the pattern descriptors\n",
        "        one_hot_columns = ['Pattern_Stationary', 'Pattern_Slow Movement', 'Pattern_High Speed']\n",
        "\n",
        "        # Step 3: Create sliding windows for sequences\n",
        "        for start in range(0, len(group) - window_size - prediction_horizon + 1):\n",
        "            # X_k, input sequence: past `window_size` time steps\n",
        "            X_seq = group.iloc[start:start + window_size][['LAT', 'LON', 'SOG'] + one_hot_columns].values\n",
        "\n",
        "            # Y_k,h output sequence: next `prediction_horizon` time steps\n",
        "            Y_seq = group.iloc[start + window_size:start + window_size + prediction_horizon][['LAT', 'LON', 'SOG']].values\n",
        "\n",
        "            # Append the sequences and the journey descriptor\n",
        "            all_sequences_X.append(X_seq)\n",
        "            all_sequences_Y.append(Y_seq)\n",
        "            all_journey_descriptors.append(group['PatternDescriptor'].iloc[start + window_size])\n",
        "\n",
        "    # Convert sequences into numpy arrays for model training\n",
        "    all_sequences_X = np.array(all_sequences_X)\n",
        "    all_sequences_Y = np.array(all_sequences_Y)\n",
        "    all_journey_descriptors = np.array(all_journey_descriptors)\n",
        "\n",
        "    # Step 4: Perform K-Fold Cross Validation\n",
        "    for train_index, val_index in kf.split(all_sequences_X):\n",
        "        # Split into training and validation sets for each fold\n",
        "        X_train, X_val = all_sequences_X[train_index], all_sequences_X[val_index]\n",
        "        Y_train, Y_val = all_sequences_Y[train_index], all_sequences_Y[val_index]\n",
        "\n",
        "        # Store training and validation sets for each fold\n",
        "        folds_train.append((X_train, Y_train))\n",
        "        folds_val.append((X_val, Y_val))\n",
        "\n",
        "        # Store journey descriptors for each fold\n",
        "        journey_descriptors.append(all_journey_descriptors[val_index])\n",
        "\n",
        "    return folds_train, folds_val, journey_descriptors\n",
        "\n",
        "\n",
        "# Define the window size, prediction horizon, and the number of folds\n",
        "window_size = 10  # Number of observed states\n",
        "prediction_horizon = 5  # Number of time steps to predict\n",
        "k = 5  # Number of folds\n",
        "\n",
        "# Load the data from the CSV file (already preprocessed)\n",
        "file_path = '/content/segmented_trajectories.csv'\n",
        "ais_data = pd.read_csv(file_path)\n",
        "\n",
        "# Apply K-Fold Cross-Validation to prepare data\n",
        "folds_train, folds_val, journey_descriptors = k_fold_sequence_to_sequence(\n",
        "    ais_data, window_size, prediction_horizon, k=k\n",
        ")\n",
        "\n",
        "# Print out the shapes of the sequences for the first fold\n",
        "X_train, Y_train = folds_train[0]\n",
        "X_val, Y_val = folds_val[0]\n",
        "\n",
        "print(f\"Training Input Shape: {X_train.shape}\")\n",
        "print(f\"Training Output Shape: {Y_train.shape}\")\n",
        "print(f\"Validation Input Shape: {X_val.shape}\")\n",
        "print(f\"Validation Output Shape: {Y_val.shape}\")\n"
      ],
      "metadata": {
        "id": "hsf44ZJJgrdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b820bd49-a1b4-4ec5-f558-1d92f06585ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing MMSI: 100%|██████████| 1482/1482 [00:06<00:00, 216.54MMSI/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Input Shape: (4537, 10, 6)\n",
            "Training Output Shape: (4537, 5, 3)\n",
            "Validation Input Shape: (1135, 10, 6)\n",
            "Validation Output Shape: (1135, 5, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0SwXmDkYLqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def k_fold_sequence_to_sequence(df, window_size, prediction_horizon, k=5):\n",
        "    \"\"\"\n",
        "    Apply K-Fold Cross-Validation for sequence-to-sequence data preparation with scaling.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Pandas DataFrame containing AIS data.\n",
        "    - window_size: Number of time steps in each input sequence.\n",
        "    - prediction_horizon: Number of time steps to predict.\n",
        "    - k: Number of folds for cross-validation.\n",
        "\n",
        "    Returns:\n",
        "    - folds_train: List of training data for each fold (X_train, Y_train).\n",
        "    - folds_val: List of validation data for each fold (X_val, Y_val).\n",
        "    - journey_descriptors: List of journey descriptors for validation sequences.\n",
        "    \"\"\"\n",
        "    folds_train = []\n",
        "    folds_val = []\n",
        "    journey_descriptors = []\n",
        "\n",
        "    # Group by MMSI\n",
        "    grouped = df.groupby('MMSI')\n",
        "\n",
        "    # Store all sequences\n",
        "    all_sequences_X = []\n",
        "    all_sequences_Y = []\n",
        "    all_journey_descriptors = []\n",
        "\n",
        "    # Define features for scaling\n",
        "    continuous_features = ['LAT', 'LON', 'SOG']\n",
        "\n",
        "    # Initialize scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the entire dataset\n",
        "    df_continuous = df[continuous_features]\n",
        "    scaler.fit(df_continuous)\n",
        "\n",
        "    # Process each MMSI group\n",
        "    for mmsi, group in tqdm(grouped, desc=\"Processing MMSI\", unit=\"MMSI\"):\n",
        "        group = group.sort_values(by='BaseDateTime')\n",
        "        group['BaseDateTime'] = pd.to_datetime(group['BaseDateTime'])\n",
        "\n",
        "        # Scale continuous features\n",
        "        group[continuous_features] = scaler.transform(group[continuous_features])\n",
        "\n",
        "        # Check if the group has enough data for the sequence-to-sequence model\n",
        "        if len(group) < window_size + prediction_horizon:\n",
        "            continue\n",
        "\n",
        "        # Generate sliding windows\n",
        "        for start in range(0, len(group) - window_size - prediction_horizon + 1):\n",
        "            # Input sequence\n",
        "            X_seq = group.iloc[start:start + window_size][continuous_features].values\n",
        "\n",
        "            # Output sequence\n",
        "            Y_seq = group.iloc[start + window_size:start + window_size + prediction_horizon][continuous_features].values\n",
        "\n",
        "            # Append to sequences\n",
        "            all_sequences_X.append(X_seq)\n",
        "            all_sequences_Y.append(Y_seq)\n",
        "            all_journey_descriptors.append(group['PatternDescriptor'].iloc[start + window_size])\n",
        "\n",
        "    # Convert sequences into numpy arrays\n",
        "    all_sequences_X = np.array(all_sequences_X)\n",
        "    all_sequences_Y = np.array(all_sequences_Y)\n",
        "    all_journey_descriptors = np.array(all_journey_descriptors)\n",
        "\n",
        "    # Apply K-Fold Cross Validation\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    for train_index, val_index in kf.split(all_sequences_X):\n",
        "        X_train, X_val = all_sequences_X[train_index], all_sequences_X[val_index]\n",
        "        Y_train, Y_val = all_sequences_Y[train_index], all_sequences_Y[val_index]\n",
        "        folds_train.append((X_train, Y_train))\n",
        "        folds_val.append((X_val, Y_val))\n",
        "        journey_descriptors.append(all_journey_descriptors[val_index])\n",
        "\n",
        "    return folds_train, folds_val, journey_descriptors\n",
        "\n",
        "\n",
        "# Define parameters\n",
        "window_size = 10  # Number of observed states\n",
        "prediction_horizon = 5  # Number of time steps to predict\n",
        "k = 5  # Number of folds\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/segmented_trajectories.csv'\n",
        "ais_data = pd.read_csv(file_path)\n",
        "ais_data[['Pattern_High Speed',\t'Pattern_Slow Movement']] = ais_data[['Pattern_High Speed',\t'Pattern_Slow Movement']].fillna(0)\n",
        "\n",
        "# Ensure proper datetime formatting\n",
        "ais_data['BaseDateTime'] = pd.to_datetime(ais_data['BaseDateTime'])\n",
        "\n",
        "# Check for and handle null values\n",
        "if ais_data.isnull().any().any():\n",
        "    print(\"Null values detected. Dropping rows with null values...\")\n",
        "    ais_data = ais_data.dropna()\n",
        "    print(f\"Remaining rows after dropping nulls: {len(ais_data)}\")\n",
        "\n",
        "# Apply K-Fold Cross-Validation with scaling\n",
        "folds_train, folds_val, journey_descriptors = k_fold_sequence_to_sequence(\n",
        "    ais_data, window_size, prediction_horizon, k=k\n",
        ")\n",
        "\n",
        "# Output the shapes of the sequences for the first fold\n",
        "X_train, Y_train = folds_train[0]\n",
        "X_val, Y_val = folds_val[0]\n",
        "\n",
        "print(f\"Training Input Shape: {X_train.shape}\")\n",
        "print(f\"Training Output Shape: {Y_train.shape}\")\n",
        "print(f\"Validation Input Shape: {X_val.shape}\")\n",
        "print(f\"Validation Output Shape: {Y_val.shape}\")\n",
        "\n",
        "\n",
        "# Check for NaN or infinite values in training and validation data\n",
        "print(f\"NaN in X_train: {np.isnan(X_train).any()}\")\n",
        "print(f\"NaN in Y_train: {np.isnan(Y_train).any()}\")\n",
        "print(f\"NaN in X_val: {np.isnan(X_val).any()}\")\n",
        "print(f\"NaN in Y_val: {np.isnan(Y_val).any()}\")\n",
        "\n",
        "print(f\"Infinite in X_train: {np.isinf(X_train).any()}\")\n",
        "print(f\"Infinite in Y_train: {np.isinf(Y_train).any()}\")\n",
        "print(f\"Infinite in X_val: {np.isinf(X_val).any()}\")\n",
        "print(f\"Infinite in Y_val: {np.isinf(Y_val).any()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wsWJZXhYLnT",
        "outputId": "c063c894-cfab-4e45-aa5f-d990e1b42329"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null values detected. Dropping rows with null values...\n",
            "Remaining rows after dropping nulls: 16035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing MMSI: 100%|██████████| 982/982 [00:06<00:00, 151.56MMSI/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Input Shape: (3118, 10, 3)\n",
            "Training Output Shape: (3118, 5, 3)\n",
            "Validation Input Shape: (780, 10, 3)\n",
            "Validation Output Shape: (780, 5, 3)\n",
            "NaN in X_train: False\n",
            "NaN in Y_train: False\n",
            "NaN in X_val: False\n",
            "NaN in Y_val: False\n",
            "Infinite in X_train: False\n",
            "Infinite in Y_train: False\n",
            "Infinite in X_val: False\n",
            "Infinite in Y_val: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# Define the Seq2Seq model with Attention\n",
        "class Seq2SeqWithAttention(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, latent_dim, num_layers, timesteps_input, timesteps_output):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.timesteps_input = timesteps_input\n",
        "        self.timesteps_output = timesteps_output\n",
        "\n",
        "        self.encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "        self.decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "\n",
        "        self.attention_dense = tf.keras.layers.Dense(1)\n",
        "        self.attention_softmax = tf.keras.layers.Softmax(axis=1)\n",
        "\n",
        "        self.output_dense = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "\n",
        "        # Encoder\n",
        "        encoder_outputs, state_h, state_c = self.encoder_lstm(encoder_inputs)  # (batch_size, timesteps_input, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_outputs, _, _ = self.decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])  # (batch_size, timesteps_decoder, latent_dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = self.attention_score(encoder_outputs, decoder_outputs)  # (batch_size, timesteps_decoder, timesteps_input)\n",
        "\n",
        "        # Compute context vector\n",
        "        attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)  # (batch_size, timesteps_decoder, timesteps_input, 1)\n",
        "        encoder_outputs_expanded = tf.expand_dims(encoder_outputs, axis=1)  # (batch_size, 1, timesteps_input, latent_dim)\n",
        "        context_vector = tf.reduce_sum(encoder_outputs_expanded * attention_weights_expanded, axis=2)  # (batch_size, timesteps_decoder, latent_dim)\n",
        "\n",
        "        # Combine context vector and decoder outputs\n",
        "        decoder_combined_context = tf.concat([context_vector, decoder_outputs], axis=-1)  # (batch_size, timesteps_decoder, 2 * latent_dim)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = self.output_dense(decoder_combined_context)  # (batch_size, timesteps_decoder, output_dim)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def attention_score(self, encoder_outputs, decoder_outputs):\n",
        "        timesteps_decoder = tf.shape(decoder_outputs)[1]\n",
        "\n",
        "        # Tile decoder outputs to match the encoder sequence length\n",
        "        decoder_expanded = tf.expand_dims(decoder_outputs, axis=2)  # (batch_size, timesteps_decoder, 1, latent_dim)\n",
        "        decoder_tiled = tf.tile(decoder_expanded, [1, 1, self.timesteps_input, 1])  # (batch_size, timesteps_decoder, timesteps_input, latent_dim)\n",
        "\n",
        "        # Expand encoder outputs to match decoder timesteps\n",
        "        encoder_expanded = tf.expand_dims(encoder_outputs, axis=1)  # (batch_size, 1, timesteps_input, latent_dim)\n",
        "        encoder_tiled = tf.tile(encoder_expanded, [1, timesteps_decoder, 1, 1])  # (batch_size, timesteps_decoder, timesteps_input, latent_dim)\n",
        "\n",
        "        # Concatenate encoder and decoder outputs\n",
        "        concat = tf.concat([encoder_tiled, decoder_tiled], axis=-1)  # (batch_size, timesteps_decoder, timesteps_input, 2 * latent_dim)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = self.attention_dense(concat)  # (batch_size, timesteps_decoder, timesteps_input, 1)\n",
        "        attention_scores = tf.squeeze(attention_scores, axis=-1)  # (batch_size, timesteps_decoder, timesteps_input)\n",
        "\n",
        "        # Apply softmax to calculate attention weights\n",
        "        attention_weights = self.attention_softmax(attention_scores)  # (batch_size, timesteps_decoder, timesteps_input)\n",
        "\n",
        "        return attention_weights\n",
        "\n",
        "\n",
        "\n",
        "# Parameters\n",
        "input_dim = X_train.shape[2]  # Number of features in the input sequence (e.g., 6)\n",
        "output_dim = Y_train.shape[2]  # Number of features in the output sequence (e.g., 3)\n",
        "latent_dim = 64  # Latent dimension for LSTM layers\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "timesteps_input = X_train.shape[1]  # Window size\n",
        "timesteps_output = Y_train.shape[1]  # Prediction horizon\n",
        "\n",
        "# Initialize the model\n",
        "model = Seq2SeqWithAttention(\n",
        "    input_dim=input_dim,\n",
        "    output_dim=output_dim,\n",
        "    latent_dim=latent_dim,\n",
        "    num_layers=num_layers,\n",
        "    timesteps_input=timesteps_input,\n",
        "    timesteps_output=timesteps_output\n",
        ")\n",
        "\n",
        "# Define the custom R² metric\n",
        "def r2_score(y_true, y_pred):\n",
        "    # Calculate the residual sum of squares\n",
        "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=None)\n",
        "    # Calculate the total sum of squares\n",
        "    y_true_mean = tf.reduce_mean(y_true, axis=None)\n",
        "    ss_tot = tf.reduce_sum(tf.square(y_true - y_true_mean), axis=None)\n",
        "    # Calculate R² score\n",
        "    r2 = 1 - (ss_res / (ss_tot + tf.keras.backend.epsilon()))\n",
        "    return r2\n",
        "\n",
        "\n",
        "# Compile the model with the R² metric\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[r2_score])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train, Y_train[:, :-1, :]],  # Encoder input: X_train, Decoder input: Y_train[:, :-1, :]\n",
        "    Y_train[:, 1:, :],  # Actual target sequence for training\n",
        "    epochs=15,\n",
        "    batch_size=32,\n",
        "    validation_data=([X_val, Y_val[:, :-1, :]], Y_val[:, 1:, :]),  # For validation, use the same format\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-57TvN-pOkA",
        "outputId": "ff789dff-2e4e-4eb1-9b66-a3438a046500"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "98/98 [==============================] - 5s 21ms/step - loss: 0.0770 - r2_score: 0.9099 - val_loss: 0.0233 - val_r2_score: 0.9726\n",
            "Epoch 2/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0152 - r2_score: 0.9819 - val_loss: 0.0164 - val_r2_score: 0.9829\n",
            "Epoch 3/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0099 - r2_score: 0.9887 - val_loss: 0.0090 - val_r2_score: 0.9903\n",
            "Epoch 4/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0056 - r2_score: 0.9937 - val_loss: 0.0050 - val_r2_score: 0.9943\n",
            "Epoch 5/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0034 - r2_score: 0.9960 - val_loss: 0.0033 - val_r2_score: 0.9959\n",
            "Epoch 6/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0026 - r2_score: 0.9968 - val_loss: 0.0025 - val_r2_score: 0.9967\n",
            "Epoch 7/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0021 - r2_score: 0.9974 - val_loss: 0.0022 - val_r2_score: 0.9970\n",
            "Epoch 8/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0018 - r2_score: 0.9979 - val_loss: 0.0018 - val_r2_score: 0.9976\n",
            "Epoch 9/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0015 - r2_score: 0.9982 - val_loss: 0.0015 - val_r2_score: 0.9980\n",
            "Epoch 10/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0014 - r2_score: 0.9982 - val_loss: 0.0019 - val_r2_score: 0.9976\n",
            "Epoch 11/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0014 - r2_score: 0.9983 - val_loss: 0.0014 - val_r2_score: 0.9982\n",
            "Epoch 12/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0012 - r2_score: 0.9985 - val_loss: 0.0011 - val_r2_score: 0.9985\n",
            "Epoch 13/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0010 - r2_score: 0.9987 - val_loss: 0.0016 - val_r2_score: 0.9979\n",
            "Epoch 14/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0010 - r2_score: 0.9987 - val_loss: 0.0011 - val_r2_score: 0.9985\n",
            "Epoch 15/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 9.0755e-04 - r2_score: 0.9989 - val_loss: 9.9033e-04 - val_r2_score: 0.9987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79FR-ay9YLdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model and capture loss and metrics\n",
        "test_results = model.evaluate([X_val, Y_val[:, :-1, :]], Y_val[:, 1:, :], verbose=1)\n",
        "test_loss, test_r2 = test_results\n",
        "\n",
        "# Print test loss and R² score\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test R² Score (Accuracy): {test_r2}\")"
      ],
      "metadata": {
        "id": "R2gZFfuFYLbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51abeaec-1029-4ccc-b4ba-7cd924afa8f3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 0s 5ms/step - loss: 9.9033e-04 - r2_score: 0.9987\n",
            "Test Loss: 0.0009903283789753914\n",
            "Test R² Score (Accuracy): 0.9987214803695679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qAQdNSKLYLY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HelVDtZjulFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define the Seq2Seq model with Attention\n",
        "class Seq2SeqWithAttention(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, latent_dim, num_layers, timesteps_input, timesteps_output):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.timesteps_input = timesteps_input\n",
        "        self.timesteps_output = timesteps_output\n",
        "\n",
        "        self.encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "        self.decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "\n",
        "        self.attention_dense = tf.keras.layers.Dense(1)\n",
        "        self.attention_softmax = tf.keras.layers.Softmax(axis=1)\n",
        "\n",
        "        self.output_dense = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "\n",
        "        # Encoder\n",
        "        encoder_outputs, state_h, state_c = self.encoder_lstm(encoder_inputs)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_outputs, _, _ = self.decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = self.attention_score(encoder_outputs, decoder_outputs)\n",
        "\n",
        "        # Compute context vector\n",
        "        attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)\n",
        "        encoder_outputs_expanded = tf.expand_dims(encoder_outputs, axis=1)\n",
        "        context_vector = tf.reduce_sum(encoder_outputs_expanded * attention_weights_expanded, axis=2)\n",
        "\n",
        "        # Combine context vector and decoder outputs\n",
        "        decoder_combined_context = tf.concat([context_vector, decoder_outputs], axis=-1)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = self.output_dense(decoder_combined_context)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def attention_score(self, encoder_outputs, decoder_outputs):\n",
        "        timesteps_decoder = tf.shape(decoder_outputs)[1]\n",
        "\n",
        "        # Tile decoder outputs to match the encoder sequence length\n",
        "        decoder_expanded = tf.expand_dims(decoder_outputs, axis=2)\n",
        "        decoder_tiled = tf.tile(decoder_expanded, [1, 1, self.timesteps_input, 1])\n",
        "\n",
        "        # Expand encoder outputs to match decoder timesteps\n",
        "        encoder_expanded = tf.expand_dims(encoder_outputs, axis=1)\n",
        "        encoder_tiled = tf.tile(encoder_expanded, [1, timesteps_decoder, 1, 1])\n",
        "\n",
        "        # Concatenate encoder and decoder outputs\n",
        "        concat = tf.concat([encoder_tiled, decoder_tiled], axis=-1)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = self.attention_dense(concat)\n",
        "        attention_scores = tf.squeeze(attention_scores, axis=-1)\n",
        "\n",
        "        # Apply softmax to calculate attention weights\n",
        "        attention_weights = self.attention_softmax(attention_scores)\n",
        "\n",
        "        return attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"input_dim\": self.output_dense.units,\n",
        "            \"output_dim\": self.timesteps_output,\n",
        "            \"latent_dim\": self.encoder_lstm.units,\n",
        "            \"num_layers\": 1,  # Modify as needed\n",
        "            \"timesteps_input\": self.timesteps_input,\n",
        "            \"timesteps_output\": self.timesteps_output,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "# Define the custom R² metric\n",
        "def r2_score(y_true, y_pred):\n",
        "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=None)\n",
        "    y_true_mean = tf.reduce_mean(y_true, axis=None)\n",
        "    ss_tot = tf.reduce_sum(tf.square(y_true - y_true_mean), axis=None)\n",
        "    r2 = 1 - (ss_res / (ss_tot + tf.keras.backend.epsilon()))\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "6uYP3aVDulCd"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parameters\n",
        "input_dim = X_train.shape[2]\n",
        "\n",
        "# Ensure output_dim matches the target data\n",
        "output_dim = Y_train.shape[2]\n",
        "\n",
        "# Initialize the model\n",
        "model = Seq2SeqWithAttention(\n",
        "    input_dim=X_train.shape[2],\n",
        "    output_dim=output_dim,\n",
        "    latent_dim=latent_dim,\n",
        "    num_layers=num_layers,\n",
        "    timesteps_input=X_train.shape[1],\n",
        "    timesteps_output=Y_train.shape[1]\n",
        ")\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[r2_score])\n",
        "model.fit([X_train, Y_train[:, :-1, :]], Y_train[:, 1:, :], epochs=15, batch_size=32,\n",
        "          validation_data=([X_val, Y_val[:, :-1, :]], Y_val[:, 1:, :]), verbose=1)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"./seq2seq_with_attention_model\", save_format=\"tf\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QauqH5ZFulAX",
        "outputId": "03a57106-df0c-47b5-ff93-c598332dce00"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "98/98 [==============================] - 5s 20ms/step - loss: 0.1607 - r2_score: 0.8106 - val_loss: 0.0426 - val_r2_score: 0.9559\n",
            "Epoch 2/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0232 - r2_score: 0.9743 - val_loss: 0.0263 - val_r2_score: 0.9700\n",
            "Epoch 3/15\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 0.0150 - r2_score: 0.9824 - val_loss: 0.0157 - val_r2_score: 0.9825\n",
            "Epoch 4/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0085 - r2_score: 0.9904 - val_loss: 0.0090 - val_r2_score: 0.9898\n",
            "Epoch 5/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0054 - r2_score: 0.9937 - val_loss: 0.0055 - val_r2_score: 0.9936\n",
            "Epoch 6/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0037 - r2_score: 0.9956 - val_loss: 0.0040 - val_r2_score: 0.9948\n",
            "Epoch 7/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0031 - r2_score: 0.9961 - val_loss: 0.0031 - val_r2_score: 0.9961\n",
            "Epoch 8/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0025 - r2_score: 0.9970 - val_loss: 0.0027 - val_r2_score: 0.9966\n",
            "Epoch 9/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0021 - r2_score: 0.9973 - val_loss: 0.0026 - val_r2_score: 0.9967\n",
            "Epoch 10/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0019 - r2_score: 0.9977 - val_loss: 0.0018 - val_r2_score: 0.9975\n",
            "Epoch 11/15\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.0016 - r2_score: 0.9980 - val_loss: 0.0015 - val_r2_score: 0.9980\n",
            "Epoch 12/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0015 - r2_score: 0.9982 - val_loss: 0.0016 - val_r2_score: 0.9979\n",
            "Epoch 13/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0015 - r2_score: 0.9981 - val_loss: 0.0014 - val_r2_score: 0.9981\n",
            "Epoch 14/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0012 - r2_score: 0.9985 - val_loss: 0.0012 - val_r2_score: 0.9983\n",
            "Epoch 15/15\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 0.0011 - r2_score: 0.9986 - val_loss: 0.0013 - val_r2_score: 0.9984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure to use consistent output_dim during reinitialization\n",
        "loaded_model = tf.keras.models.load_model(\n",
        "    \"./seq2seq_with_attention_model\",\n",
        "    custom_objects={\"r2_score\": r2_score}\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_results = loaded_model.evaluate([X_val, Y_val[:, :-1, :]], Y_val[:, 1:, :], verbose=1)\n",
        "\n",
        "# Print results\n",
        "print(f\"Test Loss: {test_results[0]}\")\n",
        "print(f\"Test R² Score: {test_results[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUfG7RVIuk-Z",
        "outputId": "1c3d329b-d6f5-452e-a04d-4f1366016aec"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 1s 6ms/step - loss: 0.0013 - r2_score: 0.9984\n",
            "Test Loss: 0.0012528256047517061\n",
            "Test R² Score: 0.9983580112457275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mcla3Dtcuk8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_IS3ngGuk51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C9MyKEJuuk3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXXlDC8zuk1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b17e42_gukzG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}